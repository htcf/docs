{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"High Throughput Computing Facility @ CGS_SB The Center for Genome Sciences and Systems Biology is the proud home of The High Throughput Computing Facility, a Washington University recharge center . The HTCF provides high-throughput computational resources for researchers within the CGS_SB. An HTCF description for grant writing purposes The CGSSB provides a computational cluster for high-throughput bioinformatics. The cluster consists of over 2000 processors and over 20TB of RAM. It supports the Center's Illumina sequencing platforms and real time sequencing analysis. Long term data storage is handled by our 10/40 GbE connected storage arrays. These arrays are currently over 4 petabytes in size. All user data is backed up and stored daily, weekly and monthly. A disaster recovery copy of select data is stored offsite. The HTCF also includes a 750TB high-speed distributed file system that is capable of throughput up to 19GB/s. This, coupled with its 100Gb network backbone, delivering 10Gb of bandwidth to cluster nodes, the HTCF can provide exceptionally high-speed transfer of large amounts of data.","title":"About"},{"location":"#high-throughput-computing-facility-cgs_sb","text":"The Center for Genome Sciences and Systems Biology is the proud home of The High Throughput Computing Facility, a Washington University recharge center . The HTCF provides high-throughput computational resources for researchers within the CGS_SB. An HTCF description for grant writing purposes The CGSSB provides a computational cluster for high-throughput bioinformatics. The cluster consists of over 2000 processors and over 20TB of RAM. It supports the Center's Illumina sequencing platforms and real time sequencing analysis. Long term data storage is handled by our 10/40 GbE connected storage arrays. These arrays are currently over 4 petabytes in size. All user data is backed up and stored daily, weekly and monthly. A disaster recovery copy of select data is stored offsite. The HTCF also includes a 750TB high-speed distributed file system that is capable of throughput up to 19GB/s. This, coupled with its 100Gb network backbone, delivering 10Gb of bandwidth to cluster nodes, the HTCF can provide exceptionally high-speed transfer of large amounts of data.","title":"High Throughput Computing Facility @ CGS_SB"},{"location":"policies/","text":"Policies WUSTL Computer Use http://wustl.edu/policies/compolicy.html Account Usage As stated in the above WUSTL Policy: \"Do not use the password of others or access files under false identity.\" Accounts and passwords cannot be shared. All users must have their own account. Account Renewal HTCF user accounts are automatically renewed annually from the original activation date unless otherwise instructed. Account Removal Home directories of expired accounts are removed 90 days after expiration. Storage Policies Scratch Data Cleaning In order to ensure top performance of /scratch it is important to clean it regularly to remove stale data. Therefore, the following weekly automated tasks are performed on /scratch: User files on scratch that have not been modified for more than 60 days are garbage collected and placed in a \u201ctrash\u201d location. After 30 days in the trash location, user files are purged from the system. Once purged, there is no way files can be restored. Please ensure that any files you need for more than 60 days are safely copied to an LTS bucket. Garbage-collected files are stored in /scratch/trash/<date_of_collection>/. You can restore your garbage-collected files by moving them out of this directory. A list of your garbage-collected files can be found in /scratch/trash/<date_of_collection>/filelists/<username>. The HTCF is not responsible for data loss from automated scrubs. Labs are responsible for monitoring their files and transferring their data from scratch to long term storage. Data Limits Each member of the HTCF belongs to at least two Unix groups. The primary group is your personal group, having the same name as your HTCF username. The secondary group is the laboratory or similar entity that you are primarily associated with. Policy: Scratch user data limits Size Limit - 2TB Inode Limit (Number of files) - 2,000,000 Example Username: johnsmith To determine your personal scratch usage: $ beegfs-ctl --getquota --uid johnsmith Login Node Policy The HTCF login node is to be used for job composition, software installation, and staging of job data. Any computational processes found running longer than 30 minutes can be terminated. General Availability Effort will be made to keep our resources available. Although the support personnel will do their best to keep the facility running at all times, we cannot guarantee to promptly resolve problems outside office hours, during weekends, and public holidays. Nevertheless, please notify us of whenever they arise. General Maintenance Occasionally, it is necessary as part of maintaining a reliable service to update system software and replace faulty hardware. Sometimes it will be possible to perform these tasks transparently by means of queue reconfiguration in a way that will not disrupt running jobs or interactive use, or significantly inconvenience users. Some tasks however, particularly those affecting storage or login nodes, may require temporary interruption of service. Running Jobs Jobs that improperly perform excessive I/O, or utilize unreserved CPU time will be terminated. Please be accurate when requesting memory, not requesting enough memory will result in your process crashing, requesting too much memory will prevent other users from running jobs.","title":"Policies"},{"location":"policies/#policies","text":"","title":"Policies"},{"location":"policies/#wustl-computer-use","text":"http://wustl.edu/policies/compolicy.html","title":"WUSTL Computer Use"},{"location":"policies/#account-usage","text":"As stated in the above WUSTL Policy: \"Do not use the password of others or access files under false identity.\" Accounts and passwords cannot be shared. All users must have their own account.","title":"Account Usage"},{"location":"policies/#account-renewal","text":"HTCF user accounts are automatically renewed annually from the original activation date unless otherwise instructed.","title":"Account Renewal"},{"location":"policies/#account-removal","text":"Home directories of expired accounts are removed 90 days after expiration.","title":"Account Removal"},{"location":"policies/#storage-policies","text":"","title":"Storage Policies"},{"location":"policies/#scratch-data-cleaning","text":"In order to ensure top performance of /scratch it is important to clean it regularly to remove stale data. Therefore, the following weekly automated tasks are performed on /scratch: User files on scratch that have not been modified for more than 60 days are garbage collected and placed in a \u201ctrash\u201d location. After 30 days in the trash location, user files are purged from the system. Once purged, there is no way files can be restored. Please ensure that any files you need for more than 60 days are safely copied to an LTS bucket. Garbage-collected files are stored in /scratch/trash/<date_of_collection>/. You can restore your garbage-collected files by moving them out of this directory. A list of your garbage-collected files can be found in /scratch/trash/<date_of_collection>/filelists/<username>. The HTCF is not responsible for data loss from automated scrubs. Labs are responsible for monitoring their files and transferring their data from scratch to long term storage.","title":"Scratch Data Cleaning"},{"location":"policies/#data-limits","text":"Each member of the HTCF belongs to at least two Unix groups. The primary group is your personal group, having the same name as your HTCF username. The secondary group is the laboratory or similar entity that you are primarily associated with. Policy: Scratch user data limits Size Limit - 2TB Inode Limit (Number of files) - 2,000,000 Example Username: johnsmith To determine your personal scratch usage: $ beegfs-ctl --getquota --uid johnsmith","title":"Data Limits"},{"location":"policies/#login-node-policy","text":"The HTCF login node is to be used for job composition, software installation, and staging of job data. Any computational processes found running longer than 30 minutes can be terminated.","title":"Login Node Policy"},{"location":"policies/#general-availability","text":"Effort will be made to keep our resources available. Although the support personnel will do their best to keep the facility running at all times, we cannot guarantee to promptly resolve problems outside office hours, during weekends, and public holidays. Nevertheless, please notify us of whenever they arise.","title":"General Availability"},{"location":"policies/#general-maintenance","text":"Occasionally, it is necessary as part of maintaining a reliable service to update system software and replace faulty hardware. Sometimes it will be possible to perform these tasks transparently by means of queue reconfiguration in a way that will not disrupt running jobs or interactive use, or significantly inconvenience users. Some tasks however, particularly those affecting storage or login nodes, may require temporary interruption of service.","title":"General Maintenance"},{"location":"policies/#running-jobs","text":"Jobs that improperly perform excessive I/O, or utilize unreserved CPU time will be terminated. Please be accurate when requesting memory, not requesting enough memory will result in your process crashing, requesting too much memory will prevent other users from running jobs.","title":"Running Jobs"},{"location":"prerequisites/","text":"A firm grasp of the following concepts and technologies are expected for users of the HTCF: Bash Shell https://www.gnu.org/software/bash/manual/html_node/index.html Bash Environment Variables https://linuxhint.com/bash-environment-variables/ Python Virtual Environments https://docs.python.org/3/tutorial/venv.html https://docs.python.org/3/library/venv.html, https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/","title":"Prerequisites"},{"location":"prerequisites/#bash-shell","text":"https://www.gnu.org/software/bash/manual/html_node/index.html","title":"Bash Shell"},{"location":"prerequisites/#bash-environment-variables","text":"https://linuxhint.com/bash-environment-variables/","title":"Bash Environment Variables"},{"location":"prerequisites/#python-virtual-environments","text":"https://docs.python.org/3/tutorial/venv.html https://docs.python.org/3/library/venv.html, https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/","title":"Python Virtual Environments"},{"location":"runningjobs/","text":"Many times you might have a command that you\u2019d like to run on a lot of samples. To run these sequentially, you might have a script like this: #!/bin/bash module load spades spades.py --careful --pe1-1 samp1-r1.fastq --pe1-2 samp1-r2.fastq -o assembly_1 spades.py --careful --pe1-1 samp2-r1.fastq --pe1-2 samp2-r2.fastq -o assembly_2 \u2026 spades.py --careful --pe1-1 samp2000-r1.fastq --pe1-2 samp2000-r2.fastq -o assembly_2000 By submitting this on the HTCF as an \u201carray job\u201d, you have the potential of running each of these commands in parallel (all at the same time) rather than sequentially (one at a time). The best case scenario could be that all 2000 commands finish in the time it takes 1 command to run! More information regarding SLURM job arrays is available at http://slurm.schedmd.com/job_array.html . Run a command or set of commands on file names that are sequentially numbered Input File 1 Input File 2 sample1-r1.fastq sample1-r2.fastq sample2-r1.fastq sample2-r2.fastq \u2026 \u2026 sample2000-r1.fastq sample2000-r2.fastq Step 1: Create an sbatch file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = ${ SLURM_ARRAY_TASK_ID } # Number between 1 and 2000 spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID } Run a command or set of commands on file names that aren\u2019t numbered sequentially Input File 1 Input File 2 sampleAAA-r1.fastq sampleAAA-r2.fastq sampleAAB-r1.fastq sampleAAB-r2.fastq \u2026 \u2026 SampleZZZ-r1.fastq SampleZZZ-r2.fastq Step 1: Create a \u201clookup\u201d file, lookup.txt AAA AAB \u2026 ZZZ Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 2: Create an sbatch file, grabbing the \u201cID\u201d in line $SLURM_ARRAY_TASK_ID from the lookup file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = $( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID } Run a command or set of commands using a complex lookup file Step 1: Create a \u201clookup\u201d file, lookup.txt 5 sampleAAA_R1 0.0001 5000 5 sampleAAA_R2 0.0001 5000 5 sampleBBB_R1 0.0005 1000 5 sampleBBB_R2 0.0005 1000 Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 3: Create an sbatch file, each job is created using the template with information from the lookup file. #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load seqtk read part1 part2 part3 part4 < < ( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) seqtk sample -s ${ part1 } /path/to/file. ${ part2 } .fastq ${ part3 } > /path/to/file. ${ part2 } _ ${ part4 } .fastq","title":"Runningjobs"},{"location":"runningjobs/#run-a-command-or-set-of-commands-on-file-names-that-are-sequentially-numbered","text":"Input File 1 Input File 2 sample1-r1.fastq sample1-r2.fastq sample2-r1.fastq sample2-r2.fastq \u2026 \u2026 sample2000-r1.fastq sample2000-r2.fastq Step 1: Create an sbatch file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = ${ SLURM_ARRAY_TASK_ID } # Number between 1 and 2000 spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID }","title":"Run a command or set of commands on file names that are sequentially numbered"},{"location":"runningjobs/#run-a-command-or-set-of-commands-on-file-names-that-arent-numbered-sequentially","text":"Input File 1 Input File 2 sampleAAA-r1.fastq sampleAAA-r2.fastq sampleAAB-r1.fastq sampleAAB-r2.fastq \u2026 \u2026 SampleZZZ-r1.fastq SampleZZZ-r2.fastq Step 1: Create a \u201clookup\u201d file, lookup.txt AAA AAB \u2026 ZZZ Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 2: Create an sbatch file, grabbing the \u201cID\u201d in line $SLURM_ARRAY_TASK_ID from the lookup file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = $( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID }","title":"Run a command or set of commands on file names that aren\u2019t numbered sequentially"},{"location":"runningjobs/#run-a-command-or-set-of-commands-using-a-complex-lookup-file","text":"Step 1: Create a \u201clookup\u201d file, lookup.txt 5 sampleAAA_R1 0.0001 5000 5 sampleAAA_R2 0.0001 5000 5 sampleBBB_R1 0.0005 1000 5 sampleBBB_R2 0.0005 1000 Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 3: Create an sbatch file, each job is created using the template with information from the lookup file. #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load seqtk read part1 part2 part3 part4 < < ( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) seqtk sample -s ${ part1 } /path/to/file. ${ part2 } .fastq ${ part3 } > /path/to/file. ${ part2 } _ ${ part4 } .fastq","title":"Run a command or set of commands using a complex lookup file"},{"location":"software/","text":"Software The HTCF starts as somewhat of a blank slate for each lab. However, this doesn't mean labs can't get up and running in a matter of minutes! Each lab has its own dedicated space to install and manage software. This reference space is located in /ref/<lab>/software . Software building and installation on the HTCF is primarily self-service. Labs are free to use their /ref software directory to install software using whatever means is most comfortable. At the lab level, use of Spack to install common software is encouraged . Virtual environments can also be used if the software is well suited. Spack Spack is a package management tool designed to support multiple versions and configurations of software on a wide variety of platforms and environments. Note Do not install software while on the login node. Please build/install software from an interactive job. Tutorial See the official spack tutorial Initialization To create a lab instance of the spack package manager: Download and untar a spack release into /ref/<lab_name>/software Rename (or make a symlink from) /ref/<lab_name>/software/spack-VERSION to /ref/<lab_name>/software/spack . Logout and log back in. This will ensure the spack command is available in the PATH. Installing Software see the official spack documentation Warning Please install software from within a Slurm job. Note Some compiling requires large amounts of RAM. Using --mem-per-cpu with >= 4G is sometimes needed. In the case of software that requires the \"qt\" package, more than --mem-per-cpu=10G could be needed. Note When installing within a slurm job, be sure to tell Spack how many CPUs are available. For example, after getting an interactive session : spack install -j ${SLURM_CPUS_PER_TASK} ..... Note Some software can take an extremely long time to install (such as qt and llvm). In these cases, an sbatch job will be needed rather than an interactive job: #!/bin/bash #SBATCH -c <num> #SBATCH --mem-per-cpu=<num>G spack install -j ${SLURM_CPUS_PER_TASK} .... Using the software Once spack has built software, the bash shell needs to have the proper environment variables set to access the software. This is accomplished using the spack load command. Spack packages can be \"loaded\" similar to the way modules are loaded. Given a spec , a spack command can be used to generate the appropriate environment variables to \"load\" spack-installed software. To set the environment variables (similar to module load ... ): $ eval $( spack load --sh <spec> ) To unset (unload) these variables: $ eval $( spack unload --sh <spec> ) To simply view the environment variables that would be set without actually setting them: $ spack load --sh <spec> See the official spack documentation for more information on specs. These commands can be placed in an sbatch file to be used in a job. #!/bin/bash eval $( spack load --sh <spec> ) Example Installing biom-format Search for the name of the package: $ spack list biom ==> 7 packages. microbiomeutil py-biomine r-biomart r-biomformat py-biom-format r-biom-utils r-biomartr See what versions are available: $ spack versions py-biom-format ==> Safe versions (already checksummed): 2.1.10 2.1.9 2.1.7 2.1.6 ... Install: $ spack install py-biom-format@2.1.10 Load the software in a job: #!/bin/bash eval $( spack load --sh py-biom-format@2.1.10 ) ... What if the software I want is not available through Spack When needed software is not readily accessible via Spack, there are a few options. Follow the installation instructions from the software creator. Sometimes, this can be very quick and straightforward. Sometimes, this can be very painful. Sometimes, it can be a good idea to pass judgement on the quality of software based on the quality of the installation process and documentation. Create a custom spack package Spack can be a wonderful tool for creating and maintaining software. Plenty of documentation is provided for creating and maintaining custom packages , though a firm understanding of python is needed. R considerations Please see the R page for more information. Manual Installation Sometimes it's just easier to follow the installation steps provided by software creator. If the software depends on other software, it might be that the dependency software could be installed via spack . For example, if a piece of software is not available via Spack, but requires samtools to be installed: # Install samtools via spack $ spack install samtools # load samtools before installation $ eval $( spack load --sh samtools ) (proceed with the manual installation) Remember The best place to install software is in reference storage: /ref/<lab>/software . After manual software installation, it's good practice to then create a module file Manual module files Module files that are manually created go in reference storage in /ref/<lab>/software/modules . The lmod documentation is the best place to learn about creating module files. What about conda? Feel free to use conda if it is required/preferred. The HTCF does not handle Conda support requests. For conda support, please see https://docs.conda.io/en/latest/help-support.html","title":"Getting Started"},{"location":"software/#software","text":"The HTCF starts as somewhat of a blank slate for each lab. However, this doesn't mean labs can't get up and running in a matter of minutes! Each lab has its own dedicated space to install and manage software. This reference space is located in /ref/<lab>/software . Software building and installation on the HTCF is primarily self-service. Labs are free to use their /ref software directory to install software using whatever means is most comfortable. At the lab level, use of Spack to install common software is encouraged . Virtual environments can also be used if the software is well suited.","title":"Software"},{"location":"software/#spack","text":"Spack is a package management tool designed to support multiple versions and configurations of software on a wide variety of platforms and environments. Note Do not install software while on the login node. Please build/install software from an interactive job.","title":"Spack"},{"location":"software/#tutorial","text":"See the official spack tutorial","title":"Tutorial"},{"location":"software/#initialization","text":"To create a lab instance of the spack package manager: Download and untar a spack release into /ref/<lab_name>/software Rename (or make a symlink from) /ref/<lab_name>/software/spack-VERSION to /ref/<lab_name>/software/spack . Logout and log back in. This will ensure the spack command is available in the PATH.","title":"Initialization"},{"location":"software/#installing-software","text":"see the official spack documentation Warning Please install software from within a Slurm job. Note Some compiling requires large amounts of RAM. Using --mem-per-cpu with >= 4G is sometimes needed. In the case of software that requires the \"qt\" package, more than --mem-per-cpu=10G could be needed. Note When installing within a slurm job, be sure to tell Spack how many CPUs are available. For example, after getting an interactive session : spack install -j ${SLURM_CPUS_PER_TASK} ..... Note Some software can take an extremely long time to install (such as qt and llvm). In these cases, an sbatch job will be needed rather than an interactive job: #!/bin/bash #SBATCH -c <num> #SBATCH --mem-per-cpu=<num>G spack install -j ${SLURM_CPUS_PER_TASK} ....","title":"Installing Software"},{"location":"software/#using-the-software","text":"Once spack has built software, the bash shell needs to have the proper environment variables set to access the software. This is accomplished using the spack load command. Spack packages can be \"loaded\" similar to the way modules are loaded. Given a spec , a spack command can be used to generate the appropriate environment variables to \"load\" spack-installed software. To set the environment variables (similar to module load ... ): $ eval $( spack load --sh <spec> ) To unset (unload) these variables: $ eval $( spack unload --sh <spec> ) To simply view the environment variables that would be set without actually setting them: $ spack load --sh <spec> See the official spack documentation for more information on specs. These commands can be placed in an sbatch file to be used in a job. #!/bin/bash eval $( spack load --sh <spec> )","title":"Using the software"},{"location":"software/#example","text":"Installing biom-format Search for the name of the package: $ spack list biom ==> 7 packages. microbiomeutil py-biomine r-biomart r-biomformat py-biom-format r-biom-utils r-biomartr See what versions are available: $ spack versions py-biom-format ==> Safe versions (already checksummed): 2.1.10 2.1.9 2.1.7 2.1.6 ... Install: $ spack install py-biom-format@2.1.10 Load the software in a job: #!/bin/bash eval $( spack load --sh py-biom-format@2.1.10 ) ...","title":"Example"},{"location":"software/#what-if-the-software-i-want-is-not-available-through-spack","text":"When needed software is not readily accessible via Spack, there are a few options. Follow the installation instructions from the software creator. Sometimes, this can be very quick and straightforward. Sometimes, this can be very painful. Sometimes, it can be a good idea to pass judgement on the quality of software based on the quality of the installation process and documentation. Create a custom spack package Spack can be a wonderful tool for creating and maintaining software. Plenty of documentation is provided for creating and maintaining custom packages , though a firm understanding of python is needed.","title":"What if the software I want is not available through Spack"},{"location":"software/#r-considerations","text":"Please see the R page for more information.","title":"R considerations"},{"location":"software/#manual-installation","text":"Sometimes it's just easier to follow the installation steps provided by software creator. If the software depends on other software, it might be that the dependency software could be installed via spack . For example, if a piece of software is not available via Spack, but requires samtools to be installed: # Install samtools via spack $ spack install samtools # load samtools before installation $ eval $( spack load --sh samtools ) (proceed with the manual installation) Remember The best place to install software is in reference storage: /ref/<lab>/software . After manual software installation, it's good practice to then create a module file","title":"Manual Installation"},{"location":"software/#manual-module-files","text":"Module files that are manually created go in reference storage in /ref/<lab>/software/modules . The lmod documentation is the best place to learn about creating module files.","title":"Manual module files"},{"location":"software/#what-about-conda","text":"Feel free to use conda if it is required/preferred. The HTCF does not handle Conda support requests. For conda support, please see https://docs.conda.io/en/latest/help-support.html","title":"What about conda?"},{"location":"software_old/","text":"Modules Lmod is a Lua based module system that easily handles the MODULEPATH Hierarchical problem. Environment Modules provide a convenient way to dynamically change the users' environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found. Software is handled using lmod . There should be minimal need to modify your .bashrc or .profile unless you're installing software locally to test. Basics Lmod is managed using the command module , using this command without options will show you a list of all available subcommands. ~$ module Modules based on Lua: Version 6.6 2016-10-13 13:28 -05:00 by Robert McLay mclay@tacc.utexas.edu module [options] sub-command [args ...] Help sub-commands: ------------------ help prints this message help module [...] print help message from module(s) A list of software available the command: ~$ module avail ---------------------------------------------- /opt/apps/modules ---------------------------------------------- GD/2.1.1 genometools/1.5.8 pindel/0.2.5b6 R/2.15.3 ghostscript/9.19 pmap/11-25-2010 R/3.1.2 glimmer/3.02b poretools/0.6.0 To load the latest (default) version of module: module load ncbi-blast To specifiy which version of the module you would like to use: module load ncbi-blast/2.2.30+ Be sure to specify which version of the sofware you'd like to use in your scripts to ensure consistent results, as software updates may break pipelines. Software with prerequisites are loaded dynamically, for example: ~$ module load prokka ~$ ml Currently Loaded Modules: 1) aragorn/1.2.36 3) prodigal/2.6.2 5) tbl2asn/24.2 7) hmmer/3.1b1 9) prokka/1.11 2) infernal/1.1.1 4) ncbi-blast/2.2.31+ 6) bio-perl/1.6.923 8) barrnap/0.6 GUI Software As the HTCF is primarily a batch queuing system for high-throughput processing of large amounts of data, GUI application are not directly supported by the HTCF. GUI application installation and setup on the HTCF are left to the end user.","title":"Software old"},{"location":"software_old/#modules","text":"Lmod is a Lua based module system that easily handles the MODULEPATH Hierarchical problem. Environment Modules provide a convenient way to dynamically change the users' environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found. Software is handled using lmod . There should be minimal need to modify your .bashrc or .profile unless you're installing software locally to test.","title":"Modules"},{"location":"software_old/#basics","text":"Lmod is managed using the command module , using this command without options will show you a list of all available subcommands. ~$ module Modules based on Lua: Version 6.6 2016-10-13 13:28 -05:00 by Robert McLay mclay@tacc.utexas.edu module [options] sub-command [args ...] Help sub-commands: ------------------ help prints this message help module [...] print help message from module(s) A list of software available the command: ~$ module avail ---------------------------------------------- /opt/apps/modules ---------------------------------------------- GD/2.1.1 genometools/1.5.8 pindel/0.2.5b6 R/2.15.3 ghostscript/9.19 pmap/11-25-2010 R/3.1.2 glimmer/3.02b poretools/0.6.0 To load the latest (default) version of module: module load ncbi-blast To specifiy which version of the module you would like to use: module load ncbi-blast/2.2.30+ Be sure to specify which version of the sofware you'd like to use in your scripts to ensure consistent results, as software updates may break pipelines. Software with prerequisites are loaded dynamically, for example: ~$ module load prokka ~$ ml Currently Loaded Modules: 1) aragorn/1.2.36 3) prodigal/2.6.2 5) tbl2asn/24.2 7) hmmer/3.1b1 9) prokka/1.11 2) infernal/1.1.1 4) ncbi-blast/2.2.31+ 6) bio-perl/1.6.923 8) barrnap/0.6","title":"Basics"},{"location":"software_old/#gui-software","text":"As the HTCF is primarily a batch queuing system for high-throughput processing of large amounts of data, GUI application are not directly supported by the HTCF. GUI application installation and setup on the HTCF are left to the end user.","title":"GUI Software"},{"location":"software/examples/r_with_mpi/","text":"Example: R+MPI The following is an example of preparing for and running a job consisting of an R script that requires: mpi ( Rmpi ) to parallalize the work bioinformatics packages sva and edgeR Install the Spack managed software (R, mpich, Rmpi) Get an interactive session with plenty of resources to help install software quickly: srun --mem-per-cpu=4G -c 8 -J interactive -p interactive --pty /bin/bash -l Install (but don't load) software that will be managed by Spack (specifying exact versions if desired): # R spack install r@<version> # mpich spack install mpich # Rmpi spack install r-rmpi ^r@<version> ^mpich Note These 3 separate commands aren't actually needed. The same could be accomplished with just 1 command: spack install r-rmpi ^r@<version> ^mpich With 3 separate commands, Spack considers all three \"explicitly\" installed. With 1 command, only r-rmpi is considered \"explicity\" installed. The rest are \"implicitly\" installed. See here for more information. Install libpng (which seems to be needed to install the sva R package later on) spack install libpng Install R libraries (sva and edgeR) Get an interactive session (if not already in one) with plenty of resources to help install software quickly: srun --mem-per-cpu=4G -c 8 -J interactive -p interactive --pty /bin/bash -l Ensure a clean environment (ie. unload any spack packages and/or modules that might be loaded from previous work): eval $(spack unload --sh --all) Load R (specifying version if desired): eval $(spack load --sh r@<version>) The sva R library seems to need libpng to be loaded: eval $(spack load --sh libpng) Decide on a shared location and name for the software and create the directory For example: mkdir /ref/jdlab/software/projecta Using an R environment variable, tell R where that location is so it will install packages there: export R_LIBS_SITE=/ref/jdlab/software/projecta Important This R_LIBS_SITE environment variable needs to be set any time R packages are installed for the project. This R_LIBS_SITE environment variable needs to be set any time the R packages need to be loaded in a job. Install sva and edgeR into R_LIBS_SITE : $ R > install.packages(\"BiocManager\") ... ... ... > BiocManager::install(\"edgeR\") ... ... ... > BiocManager::install(\"sva\") ... ... ... Put it all together in a job In the .sbatch job: #!/bin/bash #SBATCH -n X #SBATCH --mem-per-cpu=XG #SBATCH --cpus-per-task=X eval $(spack load --sh r-rmpi ^r@<version> ^mpich) export R_LIBS_SITE=/ref/jdlab/software/projecta mpiexec -usize $SLURM_NTASKS -np 1 Rscript /path/to/rscript.R","title":"Example: R+MPI"},{"location":"software/examples/r_with_mpi/#example-rmpi","text":"The following is an example of preparing for and running a job consisting of an R script that requires: mpi ( Rmpi ) to parallalize the work bioinformatics packages sva and edgeR","title":"Example: R+MPI"},{"location":"software/examples/r_with_mpi/#install-the-spack-managed-software-r-mpich-rmpi","text":"Get an interactive session with plenty of resources to help install software quickly: srun --mem-per-cpu=4G -c 8 -J interactive -p interactive --pty /bin/bash -l Install (but don't load) software that will be managed by Spack (specifying exact versions if desired): # R spack install r@<version> # mpich spack install mpich # Rmpi spack install r-rmpi ^r@<version> ^mpich Note These 3 separate commands aren't actually needed. The same could be accomplished with just 1 command: spack install r-rmpi ^r@<version> ^mpich With 3 separate commands, Spack considers all three \"explicitly\" installed. With 1 command, only r-rmpi is considered \"explicity\" installed. The rest are \"implicitly\" installed. See here for more information. Install libpng (which seems to be needed to install the sva R package later on) spack install libpng","title":"Install the Spack managed software (R, mpich, Rmpi)"},{"location":"software/examples/r_with_mpi/#install-r-libraries-sva-and-edger","text":"Get an interactive session (if not already in one) with plenty of resources to help install software quickly: srun --mem-per-cpu=4G -c 8 -J interactive -p interactive --pty /bin/bash -l Ensure a clean environment (ie. unload any spack packages and/or modules that might be loaded from previous work): eval $(spack unload --sh --all) Load R (specifying version if desired): eval $(spack load --sh r@<version>) The sva R library seems to need libpng to be loaded: eval $(spack load --sh libpng) Decide on a shared location and name for the software and create the directory For example: mkdir /ref/jdlab/software/projecta Using an R environment variable, tell R where that location is so it will install packages there: export R_LIBS_SITE=/ref/jdlab/software/projecta Important This R_LIBS_SITE environment variable needs to be set any time R packages are installed for the project. This R_LIBS_SITE environment variable needs to be set any time the R packages need to be loaded in a job. Install sva and edgeR into R_LIBS_SITE : $ R > install.packages(\"BiocManager\") ... ... ... > BiocManager::install(\"edgeR\") ... ... ... > BiocManager::install(\"sva\") ... ... ... Put it all together in a job In the .sbatch job: #!/bin/bash #SBATCH -n X #SBATCH --mem-per-cpu=XG #SBATCH --cpus-per-task=X eval $(spack load --sh r-rmpi ^r@<version> ^mpich) export R_LIBS_SITE=/ref/jdlab/software/projecta mpiexec -usize $SLURM_NTASKS -np 1 Rscript /path/to/rscript.R","title":"Install R libraries (sva and edgeR)"},{"location":"software/examples/r_mpi/","text":"Example: Using MPI with slurm via R and Rmpi Getting all the necessary software properly built, installed, and configured can be very difficult. This combination of software/versions seems to work: Spack 0.18.0 Slurm 20.11.9.1 openmpi 4.1.3 R 4.1.3 r-rmpi 0.6-9.2 Installing Step 1. Get and interactive session Step 2. Remove Slurm variables from the interactive session's environment (or Rmpi will break when it tries to install): $ for x in $(env|grep ^SLURM|cut -f1 -d=); do unset $x;done Step 3. Install r-rmpi (with a very long spec): $ spack install r-rmpi@0.6-9.2 ^r@4.1.3 ^openmpi@4.1.3 schedulers=slurm legacylaunchers=true ^slurm@20-11-9-1 Using Example sbatch file and R script .","title":"Example: Using MPI with slurm via R and Rmpi"},{"location":"software/examples/r_mpi/#example-using-mpi-with-slurm-via-r-and-rmpi","text":"Getting all the necessary software properly built, installed, and configured can be very difficult. This combination of software/versions seems to work: Spack 0.18.0 Slurm 20.11.9.1 openmpi 4.1.3 R 4.1.3 r-rmpi 0.6-9.2","title":"Example: Using MPI with slurm via R and Rmpi"},{"location":"software/examples/r_mpi/#installing","text":"Step 1. Get and interactive session Step 2. Remove Slurm variables from the interactive session's environment (or Rmpi will break when it tries to install): $ for x in $(env|grep ^SLURM|cut -f1 -d=); do unset $x;done Step 3. Install r-rmpi (with a very long spec): $ spack install r-rmpi@0.6-9.2 ^r@4.1.3 ^openmpi@4.1.3 schedulers=slurm legacylaunchers=true ^slurm@20-11-9-1","title":"Installing"},{"location":"software/examples/r_mpi/#using","text":"Example sbatch file and R script .","title":"Using"},{"location":"software/jupyter/","text":"Jupyter Lab Jupyter Lab can be installed via Spack: spack install py-jupyterlab Example sbatch script: #!/bin/bash # ------ SLURM Parameters ------ # rstudio-server is interactive. Use the interactive partition # Add cpu or memory slurm parameters as needed #SBATCH -p interactive # ------ Make sure it's run as a job ------ if [ -z \"${SLURM_JOBID}\" ]; then echo \"Error: Must be run as a job\" exit 1 fi # ------ Load environment ------ eval $(spack load --sh py-jupyterlab) port=$(shuf -i9000-9999 -n1) echo -e \" In a local terminal, create SSH tunnel to $HOSTNAME ----------------------------------------------------------------- ssh $USER@login.htcf.wustl.edu -N -L $port:$HOSTNAME:$port ----------------------------------------------------------------- Then in the desktop browser, follow the http://127.0.0.1..... address shown at the bottom of the jupyter lab command \" # Launch jupyter lab jupyter lab --no-browser --port=$port --ip=$HOSTNAME","title":"Jupyter Lab"},{"location":"software/jupyter/#jupyter-lab","text":"Jupyter Lab can be installed via Spack: spack install py-jupyterlab Example sbatch script: #!/bin/bash # ------ SLURM Parameters ------ # rstudio-server is interactive. Use the interactive partition # Add cpu or memory slurm parameters as needed #SBATCH -p interactive # ------ Make sure it's run as a job ------ if [ -z \"${SLURM_JOBID}\" ]; then echo \"Error: Must be run as a job\" exit 1 fi # ------ Load environment ------ eval $(spack load --sh py-jupyterlab) port=$(shuf -i9000-9999 -n1) echo -e \" In a local terminal, create SSH tunnel to $HOSTNAME ----------------------------------------------------------------- ssh $USER@login.htcf.wustl.edu -N -L $port:$HOSTNAME:$port ----------------------------------------------------------------- Then in the desktop browser, follow the http://127.0.0.1..... address shown at the bottom of the jupyter lab command \" # Launch jupyter lab jupyter lab --no-browser --port=$port --ip=$HOSTNAME","title":"Jupyter Lab"},{"location":"software/r/","text":"R It's best to install R using Spack, then after setting up the R search path , install R packages using the R command line interface, following the packages' install instructions. This is the best way to ensure the proper installation of the latest (or most appropriate) versions of the R libraries. Support Reminder Beyond basic R installation and basic R package installation, the HTCF cannot handle R support requests. After installation, support requests for R software and packages should be directed to: Google Software support forums or mailing lists https://www.r-project.org/help.html https://www.biostars.org/ The software developers Search paths Warning A thorough understanding of R search paths is necessary for successful installation and use of R packages. It's best to store R packages in a shared location (such as /ref/<lab>/software/... ) rather than $HOME/R . R packages in $HOME aren't as easily shared with other lab members and can quickly fill up the space in $HOME . The shared location must exist ( mkdir <dir> ) prior to starting the R command line. Please read and understand libPaths: Search Paths for Packages , specifically R_LIBS_USER and R_LIBS_SITE , before using R. It may be best to include the R version in the path such as /ref/<lab>/software/r_packages/%v For example, to set up a shared directory of R (version 4.1.X) packages and install into it: $ mkdir -p /ref/<labname>/software/r-envs/<project_name>/4.1 $ export R_LIBS_SITE=/ref/<labname>/software/r-envs/<project_name>/%v $ eval $( spack load --sh r@4.1.1 ) $ R ... > install.packages('<pkgname>') To use these R libraries in an Rscript job: #!/bin/bash export R_LIBS_SITE=/ref/<labname>/software/r-envs/<project_name>/%v eval $( spack load --sh r@4.1.1 ) Rscript ........ Troubleshooting Package Installation Unfortunately, trial and error may be required when installing R packages. Environment variables It seems some R packages such as Rsamtools and Rhtslib require extra environment variables in order to find their dependencies during install. To do this, prior to installation, the variables $LIBRARY_PATH and $C_INCLUDE_PATH can be set as follows: export LIBRARY_PATH=$_LIBRARY_PATH export C_INCLUDE_PATH=$_C_INCLUDE_PATH export CPLUS_INCLUDE_PATH=$_CPLUS_INCLUDE_PATH This should help install some R packages. Warning Most R packages DO NOT need these environment variables, and they might actually FAIL to install properly if these variables are set. Installing within rstudio Warning Some have reported problems installing software from within rstudio. It may be best to always install software from command-line R (from within an interactive job).","title":"R"},{"location":"software/r/#r","text":"It's best to install R using Spack, then after setting up the R search path , install R packages using the R command line interface, following the packages' install instructions. This is the best way to ensure the proper installation of the latest (or most appropriate) versions of the R libraries.","title":"R"},{"location":"software/r/#support","text":"Reminder Beyond basic R installation and basic R package installation, the HTCF cannot handle R support requests. After installation, support requests for R software and packages should be directed to: Google Software support forums or mailing lists https://www.r-project.org/help.html https://www.biostars.org/ The software developers","title":"Support"},{"location":"software/r/#search-paths","text":"Warning A thorough understanding of R search paths is necessary for successful installation and use of R packages. It's best to store R packages in a shared location (such as /ref/<lab>/software/... ) rather than $HOME/R . R packages in $HOME aren't as easily shared with other lab members and can quickly fill up the space in $HOME . The shared location must exist ( mkdir <dir> ) prior to starting the R command line. Please read and understand libPaths: Search Paths for Packages , specifically R_LIBS_USER and R_LIBS_SITE , before using R. It may be best to include the R version in the path such as /ref/<lab>/software/r_packages/%v For example, to set up a shared directory of R (version 4.1.X) packages and install into it: $ mkdir -p /ref/<labname>/software/r-envs/<project_name>/4.1 $ export R_LIBS_SITE=/ref/<labname>/software/r-envs/<project_name>/%v $ eval $( spack load --sh r@4.1.1 ) $ R ... > install.packages('<pkgname>') To use these R libraries in an Rscript job: #!/bin/bash export R_LIBS_SITE=/ref/<labname>/software/r-envs/<project_name>/%v eval $( spack load --sh r@4.1.1 ) Rscript ........","title":"Search paths"},{"location":"software/r/#troubleshooting-package-installation","text":"Unfortunately, trial and error may be required when installing R packages.","title":"Troubleshooting Package Installation"},{"location":"software/r/#environment-variables","text":"It seems some R packages such as Rsamtools and Rhtslib require extra environment variables in order to find their dependencies during install. To do this, prior to installation, the variables $LIBRARY_PATH and $C_INCLUDE_PATH can be set as follows: export LIBRARY_PATH=$_LIBRARY_PATH export C_INCLUDE_PATH=$_C_INCLUDE_PATH export CPLUS_INCLUDE_PATH=$_CPLUS_INCLUDE_PATH This should help install some R packages. Warning Most R packages DO NOT need these environment variables, and they might actually FAIL to install properly if these variables are set.","title":"Environment variables"},{"location":"software/r/#installing-within-rstudio","text":"Warning Some have reported problems installing software from within rstudio. It may be best to always install software from command-line R (from within an interactive job).","title":"Installing within rstudio"},{"location":"software/rstudio/","text":"RStudio Rstudio Server can be run as an interactive job and accessed via an SSH tunnel. Warning The Spack rstudio package is the Destop Version of rstudio and is NOT for use on the HTCF . Support Reminder The HTCF does not handle Rstudio support requests After installation, support requests for lab-installed software such as rstudio should be directed to: Google Software support forums or mailing lists The software developers Note The texlive package may need modifying before installing rstudio-server. If there is a texlive error during install, the following two lines should be commented out in the texlive package ( spack edit texlive ): # version('live', sha256='74eac0855e1e40c8db4f28b24ef354bd7263c1f76031bdc02b52156b572b7a1d', # url='ftp://tug.org/historic/systems/texlive/2021/install-tl-unx.tar.gz') Building and Installing Warning Compiling rstudo requires large amounts of RAM. When setting up the Slurm jobs be sure to include: --mem-per-cpu=4G --cpus-per-task=<NUM> in the Slurm parameters. ( More than 1 CPU will make the building faster but could cause longer waiting in the queue. ) A custom Spack package needs to be created: spack create rstudio-server An example rstudio-server package.py (tested against Spack-0.18.0) can be found here . Note Be sure to tell Spack how many CPUs are available: spack install -j ${SLURM_CPUS_PER_TASK} rstudio-server Running rstudio-server as a job A sample rstudio.sbatch script can be found here . Note The rstudio.sbatch script will likely need to be modified to add CPU and/or Memory requirements.","title":"RStudio"},{"location":"software/rstudio/#rstudio","text":"Rstudio Server can be run as an interactive job and accessed via an SSH tunnel. Warning The Spack rstudio package is the Destop Version of rstudio and is NOT for use on the HTCF .","title":"RStudio"},{"location":"software/rstudio/#support","text":"Reminder The HTCF does not handle Rstudio support requests After installation, support requests for lab-installed software such as rstudio should be directed to: Google Software support forums or mailing lists The software developers Note The texlive package may need modifying before installing rstudio-server. If there is a texlive error during install, the following two lines should be commented out in the texlive package ( spack edit texlive ): # version('live', sha256='74eac0855e1e40c8db4f28b24ef354bd7263c1f76031bdc02b52156b572b7a1d', # url='ftp://tug.org/historic/systems/texlive/2021/install-tl-unx.tar.gz')","title":"Support"},{"location":"software/rstudio/#building-and-installing","text":"Warning Compiling rstudo requires large amounts of RAM. When setting up the Slurm jobs be sure to include: --mem-per-cpu=4G --cpus-per-task=<NUM> in the Slurm parameters. ( More than 1 CPU will make the building faster but could cause longer waiting in the queue. ) A custom Spack package needs to be created: spack create rstudio-server An example rstudio-server package.py (tested against Spack-0.18.0) can be found here . Note Be sure to tell Spack how many CPUs are available: spack install -j ${SLURM_CPUS_PER_TASK} rstudio-server","title":"Building and Installing"},{"location":"software/rstudio/#running-rstudio-server-as-a-job","text":"A sample rstudio.sbatch script can be found here . Note The rstudio.sbatch script will likely need to be modified to add CPU and/or Memory requirements.","title":"Running rstudio-server as a job"},{"location":"storage/","text":"Storage The HTCF provides five types of storage: HDS Home Directory Storage (HDS) can be used to store scripts, development tools, etc. Home directories are located in /home/<WUSTLKEY_ID> and are available on all nodes. Home directory space is limited to 20GB. HDS is kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. Note Home directory space is not a high-speed resource like /scratch space. Please keep home directory access to a minimum in batch jobs. By default, home directories will not be readable or writeable by other users of HTCF. Feel free to change this default, if desired. To check home directory usage: $ du -sh $HOME LTS Long Term Storage (LTS) is lab project space to store raw sequencing and completed data, the directories are not available on nodes for computational use. It is available in terabyte increments billed monthly. It is kept on fault-tolerant storage with snapshops. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. To check LTS usage: $ df -h /lts/<lab_name>/<bucket_name> LTOS - Long Term Object Storage LTOS is an architecture that manages data as objects , as opposed to traditional LTS which uses file systems and block storage. LTOS is located onsite and is not Amazon S3 , but is a subset , and works in a similar way as Amazon S3. Unlike LTS, which is only accessible from the login server, LTOS is accessible externally and from all HTCF nodes. Purpose LTOS can be a good alternative to LTS any time the data in question doesn't need to be heavily manipulated or modified. Good Candidates for LTOS: Raw sequence data and finished analysis data Archived or rarely referenced data such as alumni files. Data that is not often modified, once created. Using When purchasing LTOS, please indicate whether the storage needs to be backed up offsite or only one copy is needed onsite. The per-TB cost of LTOS with an offsite copy is the same as LTS. If offsite storage is not needed, the cost is about 1/3. An \"access key\" and \"secret key\" will be assigned and can be used to connect to the storage. Buckets can be created and data can be transferred in/out using a command line s3 transfer tool. The s3cmd tool is recommended. It's available via spack as \"py-s3cmd\". After s3cmd is installed, a config file needs to be created. For LTOS with an offsite copy: cat > s3cmd-mystorage.conf <<EOF [default] host_base = s3-obs2.htcf.wustl.edu host_bucket = s3-obs2.htcf.wustl.edu access_key = ... secret_key = ... EOF For LTOS with no offsite copy: cat > s3cmd-mystorage.conf <<EOF [default] host_base = s3-obs1.htcf.wustl.edu host_bucket = s3-obs1.htcf.wustl.edu access_key = ... secret_key = ... EOF s3cmd can reference the config via a parameter: s3cmd -c s3cmd-mystorage.conf <cmd> To integrate LTOS access into software, there are many s3 API libraries for various programming languages. REF /ref is storage space for software and reference databases (such as NCBI databases or software-provided reference sequences). Each lab has an initial 1TB of reference space, and this space can be expanded at LTS prices. Note /ref is not currently backed up, therefore Any data in /ref that cannot be recreated should be copied to long term storage (LTS or LTOS) for safe-keeping. /ref \u251c\u2500\u2500 <lab> \u251c\u2500\u2500 data \u2514\u2500\u2500 software The data directory is well suited for modestly sized reference data such as NCBI blast databases . Larger datasets (> 500GB) are probably better suited for Long Term Object Storage The software directory is suited for software installation using tools such as spack HTS High Throughput Storage ( /scratch ) is a distrubuted file system able to handle tens of GBs/sec of total throughput. This storage is temporary scratch space and is not backed up. Once data is removed from /scratch, it cannot be recovered. Data stored in /scratch is subject to the Scratch Data Cleaning Policy . Jobs utilize this space for inputs and outputs to provide the best performance possible. Running jobs that read/write from the home directory will cause slowness and login issues for all users. Note Users will be asked to clean up older files on /scratch if it is needed to improve system performance. The best use of the HTS is to use a workflow similar to the following: Copy starting (raw) data from LTS to HTS. Submit jobs to the cluster that process the data, creating intermediate and/or finished data. Copy the finished data (and job files used to create that data) over to LTS. Remove all working data from HTS Results that are generated on this storage need to be promptly copied to LTS. Scratch Quotas There is a quota of 2TB per user in /scratch to prevent the filesystem from filling up. At >85% /scratch can become very slow. To check the amount of space being used, use the following command: $ beegfs-ctl --getquota --uid $USER or check group quotas with: $ beegfs-ctl --getquota --gid GROUPNAME Quota Increase Requests Note A quota increase is not guaranteed. If excess capacity is not available, a quota increase cannot be granted. If excess capacity is available. A temporary increase in the scratch quota can be requested. To request more scratch space, email following information: Reason for the increase Amount of additional space Duration additional space will be required Recommendations No important source code, scripts, libraries, executables should be kept in /scratch Do not make symlinks from the home directory to folders in /scratch Sharing Files Publicly Globus Globus can be used to share data from LTS, /scratch, or Home directories. The Globus \"Collection\" is called \" HTCF@WUSTL \". From here, selected directories can be shared as \"Guest Collections\". Please see the globus documentation for more information Note In order to share a directory, it (and all its parent directories) need to be readable by all users. For long term hosting of publicly accessible data, please contact WUSTL IT. Copying Files Using Rsync Using rsync to transfer to scratch and LTS is recommended. Rsync can resume failed copies, be re-run to ensure all of the data has been transferred, and will also transfer incremental changes. This will save a substantial amount of time if it is necessary to verify that all files have been successfully copied. When using this command, please note that the absense of a trailing slash means the directory, with a trailing slash means the contents of that directory. Here are a few examples: ~$ rsync -aHv --progress /directory/to/transfer /destination/directory/location/ The above example would put the directory named \"transfer\" into the directory named \"location\" ~$ rsync -aHv --progress /directory/to/transfer/ /destination/directory/location/ The above example would put the contents of the directory named \"transfer\" into the directory named \"location\". More info... Disk Quota Exceeded Errors If a disk quota exceeded error messages is encountered, please check each storage location to ensure there is enough disk space available.","title":"Storage"},{"location":"storage/#storage","text":"The HTCF provides five types of storage:","title":"Storage"},{"location":"storage/#hds","text":"Home Directory Storage (HDS) can be used to store scripts, development tools, etc. Home directories are located in /home/<WUSTLKEY_ID> and are available on all nodes. Home directory space is limited to 20GB. HDS is kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. Note Home directory space is not a high-speed resource like /scratch space. Please keep home directory access to a minimum in batch jobs. By default, home directories will not be readable or writeable by other users of HTCF. Feel free to change this default, if desired. To check home directory usage: $ du -sh $HOME","title":"HDS"},{"location":"storage/#lts","text":"Long Term Storage (LTS) is lab project space to store raw sequencing and completed data, the directories are not available on nodes for computational use. It is available in terabyte increments billed monthly. It is kept on fault-tolerant storage with snapshops. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. To check LTS usage: $ df -h /lts/<lab_name>/<bucket_name>","title":"LTS"},{"location":"storage/#ltos-long-term-object-storage","text":"LTOS is an architecture that manages data as objects , as opposed to traditional LTS which uses file systems and block storage. LTOS is located onsite and is not Amazon S3 , but is a subset , and works in a similar way as Amazon S3. Unlike LTS, which is only accessible from the login server, LTOS is accessible externally and from all HTCF nodes.","title":"LTOS - Long Term Object Storage"},{"location":"storage/#purpose","text":"LTOS can be a good alternative to LTS any time the data in question doesn't need to be heavily manipulated or modified. Good Candidates for LTOS: Raw sequence data and finished analysis data Archived or rarely referenced data such as alumni files. Data that is not often modified, once created.","title":"Purpose"},{"location":"storage/#using","text":"When purchasing LTOS, please indicate whether the storage needs to be backed up offsite or only one copy is needed onsite. The per-TB cost of LTOS with an offsite copy is the same as LTS. If offsite storage is not needed, the cost is about 1/3. An \"access key\" and \"secret key\" will be assigned and can be used to connect to the storage. Buckets can be created and data can be transferred in/out using a command line s3 transfer tool. The s3cmd tool is recommended. It's available via spack as \"py-s3cmd\". After s3cmd is installed, a config file needs to be created. For LTOS with an offsite copy: cat > s3cmd-mystorage.conf <<EOF [default] host_base = s3-obs2.htcf.wustl.edu host_bucket = s3-obs2.htcf.wustl.edu access_key = ... secret_key = ... EOF For LTOS with no offsite copy: cat > s3cmd-mystorage.conf <<EOF [default] host_base = s3-obs1.htcf.wustl.edu host_bucket = s3-obs1.htcf.wustl.edu access_key = ... secret_key = ... EOF s3cmd can reference the config via a parameter: s3cmd -c s3cmd-mystorage.conf <cmd> To integrate LTOS access into software, there are many s3 API libraries for various programming languages.","title":"Using"},{"location":"storage/#ref","text":"/ref is storage space for software and reference databases (such as NCBI databases or software-provided reference sequences). Each lab has an initial 1TB of reference space, and this space can be expanded at LTS prices. Note /ref is not currently backed up, therefore Any data in /ref that cannot be recreated should be copied to long term storage (LTS or LTOS) for safe-keeping. /ref \u251c\u2500\u2500 <lab> \u251c\u2500\u2500 data \u2514\u2500\u2500 software The data directory is well suited for modestly sized reference data such as NCBI blast databases . Larger datasets (> 500GB) are probably better suited for Long Term Object Storage The software directory is suited for software installation using tools such as spack","title":"REF"},{"location":"storage/#hts","text":"High Throughput Storage ( /scratch ) is a distrubuted file system able to handle tens of GBs/sec of total throughput. This storage is temporary scratch space and is not backed up. Once data is removed from /scratch, it cannot be recovered. Data stored in /scratch is subject to the Scratch Data Cleaning Policy . Jobs utilize this space for inputs and outputs to provide the best performance possible. Running jobs that read/write from the home directory will cause slowness and login issues for all users. Note Users will be asked to clean up older files on /scratch if it is needed to improve system performance. The best use of the HTS is to use a workflow similar to the following: Copy starting (raw) data from LTS to HTS. Submit jobs to the cluster that process the data, creating intermediate and/or finished data. Copy the finished data (and job files used to create that data) over to LTS. Remove all working data from HTS Results that are generated on this storage need to be promptly copied to LTS.","title":"HTS"},{"location":"storage/#scratch-quotas","text":"There is a quota of 2TB per user in /scratch to prevent the filesystem from filling up. At >85% /scratch can become very slow. To check the amount of space being used, use the following command: $ beegfs-ctl --getquota --uid $USER or check group quotas with: $ beegfs-ctl --getquota --gid GROUPNAME","title":"Scratch Quotas"},{"location":"storage/#quota-increase-requests","text":"Note A quota increase is not guaranteed. If excess capacity is not available, a quota increase cannot be granted. If excess capacity is available. A temporary increase in the scratch quota can be requested. To request more scratch space, email following information: Reason for the increase Amount of additional space Duration additional space will be required","title":"Quota Increase Requests"},{"location":"storage/#recommendations","text":"No important source code, scripts, libraries, executables should be kept in /scratch Do not make symlinks from the home directory to folders in /scratch","title":"Recommendations"},{"location":"storage/#sharing-files-publicly","text":"","title":"Sharing Files Publicly"},{"location":"storage/#globus","text":"Globus can be used to share data from LTS, /scratch, or Home directories. The Globus \"Collection\" is called \" HTCF@WUSTL \". From here, selected directories can be shared as \"Guest Collections\". Please see the globus documentation for more information Note In order to share a directory, it (and all its parent directories) need to be readable by all users. For long term hosting of publicly accessible data, please contact WUSTL IT.","title":"Globus"},{"location":"storage/#copying-files-using-rsync","text":"Using rsync to transfer to scratch and LTS is recommended. Rsync can resume failed copies, be re-run to ensure all of the data has been transferred, and will also transfer incremental changes. This will save a substantial amount of time if it is necessary to verify that all files have been successfully copied. When using this command, please note that the absense of a trailing slash means the directory, with a trailing slash means the contents of that directory. Here are a few examples: ~$ rsync -aHv --progress /directory/to/transfer /destination/directory/location/ The above example would put the directory named \"transfer\" into the directory named \"location\" ~$ rsync -aHv --progress /directory/to/transfer/ /destination/directory/location/ The above example would put the contents of the directory named \"transfer\" into the directory named \"location\". More info...","title":"Copying Files Using Rsync"},{"location":"storage/#disk-quota-exceeded-errors","text":"If a disk quota exceeded error messages is encountered, please check each storage location to ensure there is enough disk space available.","title":"Disk Quota Exceeded Errors"},{"location":"storage/compare/","text":"Storage Comparison .md-typeset table:not([class]) th:nth-child(2) { text-align: center; background-color: #4473c5; } .md-typeset table:not([class]) th:nth-child(3) { background-color: #70ad46; } .md-typeset table:not([class]) th:nth-child(4) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(4), .md-typeset table:not([class]) th:nth-child(6) { color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(5) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(6) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(6) :text { visibility: hidden; } .md-typeset table:not([class]) thead th { font-size: 1rem; } .md-typeset table:not([class]) thead th, .md-typeset table:not([class]) tbody tr:nth-child(1) td, .md-typeset table:not([class]) td:nth-child(1) { font-weight: bold; } .md-typeset table:not([class]) td:nth-child(2) { background-color: #dae3f4; } .md-typeset table:not([class]) td:nth-child(3) { background-color: #e2f0d9; } .md-typeset table:not([class]) td:nth-child(4) { background-color: #f4b184; } .md-typeset table:not([class]) td:nth-child(5) { background-color: #f8cbac; } .md-typeset table:not([class]) td:nth-child(6) { background-color: #fbe5d7; } .md-typeset table:not([class]) tr:nth-child(1) td:nth-child(2) { background-color: #8fabda; } .md-typeset table:not([class]) tr:nth-child(1) td:nth-child(3) { background-color: #a8d18d; } table.storage thead tr.sub th.hts { background-color: #a8d18d; } table.storage thead tr.sub th.lts { background-color: #f4b184; } table.storage thead tr.sub th.ltos { background-color: #f7ccac; } table.storage thead tr.sub th.ref { background-color: #fbe5d7; } table.storage tbody td.lts { background-color: #f4b184; } .md-sidebar { display: none; } .md-content { max-width: 100%; } Home HTS LTS LTS LTS Home Directory (/home) Scratch Space (/scratch) Filesystem (/lts) Object Store (LTOS) Reference (/ref) Purpose - environment customizations and scripts - development and storage of small personal project TEMPORARY STORAGE FOR: raw/initial data needed for processing by running/pending jobs intermediate data generated by running jobs final processed data prior to moving data to safer location (LTS) after jobs complete Long Term Storage For: raw/initial data such as sequencer data finished (fully processed) data documentation describing how to reproduce fully processed data from initial data Long Term Storage For: raw/initial data such as sequencer data finished (fully processed) data documentation describing how to reproduce fully processed data from initial data Long Term Storage For: Software tools needed by jobs for the processing of data reference data such as reference genomes and NCBI databases Initial Size 20G 2TB per account - - 1TB per lab Increase Cost NA $7.77/TB/Month $7.77/TB/Month $7.77/TB/Month Size Limit 20G Temporary increase available as resources allow. Please provide # TBs needed and duration of need 10 TB per bucket No limit on # of buckets - - Access from All HTCF Nodes All HTCF Nodes Login Node Only All HTCF Nodes All HTCF Nodes Access Type Standard Filesystem Standard Filesystem Standard Filesystem /lts/<lab>/<bucket> HTTP interface compatible with (but not using) Amazon S3 API Standard Filesystem /ref/<lab>/data /ref/<lab>/software Est. Access Speed Slow 10+ GB/s (aggregate) 200 MB/s 1+ GB/s (aggregate) 100 MB/s (aggregate) Backup Policy Onsite daily/weekly/monthly snapshots as resources allow Offsite backup daily NO BACKUPS Onsite daily/weekly/monthly snapshots as resources allow. Offsite backup daily. mirrored offsite. User customizable: versioning of objects, schedule removal of old objects NO BACKUPS Cleaning Policy - See the scratch data cleaning policy - - -","title":"Storage Comparison"},{"location":"storage/compare/#storage-comparison","text":".md-typeset table:not([class]) th:nth-child(2) { text-align: center; background-color: #4473c5; } .md-typeset table:not([class]) th:nth-child(3) { background-color: #70ad46; } .md-typeset table:not([class]) th:nth-child(4) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(4), .md-typeset table:not([class]) th:nth-child(6) { color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(5) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(6) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(6) :text { visibility: hidden; } .md-typeset table:not([class]) thead th { font-size: 1rem; } .md-typeset table:not([class]) thead th, .md-typeset table:not([class]) tbody tr:nth-child(1) td, .md-typeset table:not([class]) td:nth-child(1) { font-weight: bold; } .md-typeset table:not([class]) td:nth-child(2) { background-color: #dae3f4; } .md-typeset table:not([class]) td:nth-child(3) { background-color: #e2f0d9; } .md-typeset table:not([class]) td:nth-child(4) { background-color: #f4b184; } .md-typeset table:not([class]) td:nth-child(5) { background-color: #f8cbac; } .md-typeset table:not([class]) td:nth-child(6) { background-color: #fbe5d7; } .md-typeset table:not([class]) tr:nth-child(1) td:nth-child(2) { background-color: #8fabda; } .md-typeset table:not([class]) tr:nth-child(1) td:nth-child(3) { background-color: #a8d18d; } table.storage thead tr.sub th.hts { background-color: #a8d18d; } table.storage thead tr.sub th.lts { background-color: #f4b184; } table.storage thead tr.sub th.ltos { background-color: #f7ccac; } table.storage thead tr.sub th.ref { background-color: #fbe5d7; } table.storage tbody td.lts { background-color: #f4b184; } .md-sidebar { display: none; } .md-content { max-width: 100%; } Home HTS LTS LTS LTS Home Directory (/home) Scratch Space (/scratch) Filesystem (/lts) Object Store (LTOS) Reference (/ref) Purpose - environment customizations and scripts - development and storage of small personal project TEMPORARY STORAGE FOR: raw/initial data needed for processing by running/pending jobs intermediate data generated by running jobs final processed data prior to moving data to safer location (LTS) after jobs complete Long Term Storage For: raw/initial data such as sequencer data finished (fully processed) data documentation describing how to reproduce fully processed data from initial data Long Term Storage For: raw/initial data such as sequencer data finished (fully processed) data documentation describing how to reproduce fully processed data from initial data Long Term Storage For: Software tools needed by jobs for the processing of data reference data such as reference genomes and NCBI databases Initial Size 20G 2TB per account - - 1TB per lab Increase Cost NA $7.77/TB/Month $7.77/TB/Month $7.77/TB/Month Size Limit 20G Temporary increase available as resources allow. Please provide # TBs needed and duration of need 10 TB per bucket No limit on # of buckets - - Access from All HTCF Nodes All HTCF Nodes Login Node Only All HTCF Nodes All HTCF Nodes Access Type Standard Filesystem Standard Filesystem Standard Filesystem /lts/<lab>/<bucket> HTTP interface compatible with (but not using) Amazon S3 API Standard Filesystem /ref/<lab>/data /ref/<lab>/software Est. Access Speed Slow 10+ GB/s (aggregate) 200 MB/s 1+ GB/s (aggregate) 100 MB/s (aggregate) Backup Policy Onsite daily/weekly/monthly snapshots as resources allow Offsite backup daily NO BACKUPS Onsite daily/weekly/monthly snapshots as resources allow. Offsite backup daily. mirrored offsite. User customizable: versioning of objects, schedule removal of old objects NO BACKUPS Cleaning Policy - See the scratch data cleaning policy - - -","title":"Storage Comparison"},{"location":"storage/ltos/","text":"Long Term Object Storage Long Term Object Storage (LTOS) is an architecture that manages data as objects , as opposed to traditional LTS which uses file systems and block storage. Access to LTOS is REST -based (HTTP). Although LTOS uses a subset of the Amazon s3 API , LTOS data is stored internally , not in Amazon. Unlike LTS, which is only accessible from the login server, LTOS is accessible from all HTCF nodes. Purpose LTOS can be a good alternative to LTS any time the data in question doesn't need to be heavily manipulated or modified. Good Candidates for LTOS: Raw sequence data and finished analysis data Archived or rarely referenced data such as alumni files. Data that is not often modified, once created. Not Good Candidates for LTOS: scripts in use or under development software intermediate files generated during job processing Using LTOS Software Tools CLI The two most common command line tools for accessing LTOS are s3cmd and aws-cli. s3cmd First a configuration files needs to be created Workflow Putting files in LTOS Examples Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Long Term Object Storage"},{"location":"storage/ltos/#long-term-object-storage","text":"Long Term Object Storage (LTOS) is an architecture that manages data as objects , as opposed to traditional LTS which uses file systems and block storage. Access to LTOS is REST -based (HTTP). Although LTOS uses a subset of the Amazon s3 API , LTOS data is stored internally , not in Amazon. Unlike LTS, which is only accessible from the login server, LTOS is accessible from all HTCF nodes.","title":"Long Term Object Storage"},{"location":"storage/ltos/#purpose","text":"LTOS can be a good alternative to LTS any time the data in question doesn't need to be heavily manipulated or modified. Good Candidates for LTOS: Raw sequence data and finished analysis data Archived or rarely referenced data such as alumni files. Data that is not often modified, once created. Not Good Candidates for LTOS: scripts in use or under development software intermediate files generated during job processing","title":"Purpose"},{"location":"storage/ltos/#using-ltos","text":"","title":"Using LTOS"},{"location":"storage/ltos/#software-tools","text":"","title":"Software Tools"},{"location":"storage/ltos/#cli","text":"The two most common command line tools for accessing LTOS are s3cmd and aws-cli.","title":"CLI"},{"location":"storage/ltos/#s3cmd","text":"First a configuration files needs to be created","title":"s3cmd"},{"location":"storage/ltos/#workflow","text":"","title":"Workflow"},{"location":"storage/ltos/#putting-files-in-ltos","text":"","title":"Putting files in LTOS"},{"location":"storage/ltos/#examples","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Examples"},{"location":"using/","text":"Your HTCF Account Once your software is set up and your data is placed in the proper storage location you are ready to get down to work. Work is done on the HTCF via use of the Workload Management System (or Job Scheduler). Slurm Scheduler Overview Here HTCF Layout HERE (partitions) batch jobs and monitoring of running batch jobs interactive jobs job accounting","title":"Your HTCF Account"},{"location":"using/#your-htcf-account","text":"Once your software is set up and your data is placed in the proper storage location you are ready to get down to work. Work is done on the HTCF via use of the Workload Management System (or Job Scheduler). Slurm Scheduler Overview Here HTCF Layout HERE (partitions) batch jobs and monitoring of running batch jobs interactive jobs job accounting","title":"Your HTCF Account"},{"location":"using/accounting/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Accounting"},{"location":"using/batch/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Batch"},{"location":"using/getstarted/","text":"Your HTCF Account Account Creation To request a user account on the HTCF, please send an email containing the WUSTLKey username and department ID (Workday 'CC' number), for billing purposes. Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account. Logging In WUSTLKey credentials are used for authentication ( http://wustlkey.wustl.edu/ ) The login server, login.htcf.wustl.edu is accessible via ssh. Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account. First things first... Before using the HTCF, it's important to read through and understand: The HTCF Storage Using software on the HTCF Data & Data Storage Home Directories Home directories are a small, fixed amount of storage per account. They are kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. More info... Long Term Storage LTS is used to store raw and \"finished\" project data. The LTS directories are not available on the cluster nodes. It is kept on fault-tolerant storage with snapshops. More info... High Throughput Storage High Throughput Storage is a distrubuted file system able to handle tens of GBs/sec of total throughput. More info... GUI Software Note As the HTCF is primarily a batch queuing system for high-throughput processing of data, use of GUI applications are not directly supported. While use of GUI applications is possible using X forwarding, this can sometimes require significant desktop preparation and configuration which is beyond the scope of support. Jobs Resources The number of CPUs and MBs of RAM per node can be found using the Slurm sinfo command: $ sinfo -N -p general -o '%n %c %m' Interactive Interactive sessions are for running interactive scripts, vizualization, any tasks that are too computational intensive to run on the login node not submitted via sbatch. The defaults are: 1 CPU core, 1 GB RAM, and a time limit of 8 hours. Note The HTCF is primarily a batch queuing system. Interactive jobs are meant to function as daily workspaces. Because interactive jobs are by their nature, inefficient, they are not meant to be running continuously for more than 1 day. When using interactive tools such as rstudio or jupyter, please make sure the jobs are using the \"interactive\" queue (using sbatch/srun parameters -J interactive -p interactive ) Jobs using interactive tools that are not in the interactive queue will be subject to cancellation in order to free up resources for batch jobs. Tools such as Rscript can be used to run R programs in a batch fashion. It appears that jupyter notebooks can also be run in a batch fashion . Thanks for helping to ensure fairness for all folks on the HTCF. An interactive session can be started using the Slurm srun command: $ srun --mem-per-cpu=<MBs> --cpus-per-task=<num> -J interactive -p interactive --pty /bin/bash -l Batch Job Submission Determine resources Create Job File Create sbatch file with required resources Submit Monitor Workflow Jobs typically follow a generic workflow. Preprocessed Raw Data Enters LTS Raw Data is copied to scratch for processing Post processed data is copied to LTS Intermediate data generated in Step 2 is removed Sbatch Examples Create a job script (myjob.sbatch): #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem=1G eval $( spack load --sh <program> ) program /scratch/lab/files/ABC.fasta /scratch/lab/files/ABC.out Submit the sbatch script: $ sbatch myjob.sbatch View the job in the queue: $ squeue GPUs The HTCF currently has a small number of GPUs, currently 6 NVIDIA A100 80GB and 2 V100 32GB. A GPU is accessible using the following slurm parameters: -p gpu --gpus=<num> In an sbatch: #SBATCH -p gpu #SBATCH --gpus=<num> You can verify the GPU is being utilized by the job with the nvidia-smi command, this example runs within your job allocation: $ srun --ntasks-per-node=1 --jobid={jobid} nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 525.60.13 Driver Version: 525.60.13 CUDA Version: 12.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA A100 80G... Off | 00000000:17:00.0 Off | 0 | | N/A 39C P0 62W / 300W | 0MiB / 81920MiB | 23% Default | | | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 12043 C guppy_basecaller 4753MiB | +-----------------------------------------------------------------------------+","title":"Your HTCF Account"},{"location":"using/getstarted/#your-htcf-account","text":"","title":"Your HTCF Account"},{"location":"using/getstarted/#account-creation","text":"To request a user account on the HTCF, please send an email containing the WUSTLKey username and department ID (Workday 'CC' number), for billing purposes. Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account.","title":"Account Creation"},{"location":"using/getstarted/#logging-in","text":"WUSTLKey credentials are used for authentication ( http://wustlkey.wustl.edu/ ) The login server, login.htcf.wustl.edu is accessible via ssh. Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account.","title":"Logging In"},{"location":"using/getstarted/#first-things-first","text":"Before using the HTCF, it's important to read through and understand: The HTCF Storage Using software on the HTCF","title":"First things first..."},{"location":"using/getstarted/#data-data-storage","text":"","title":"Data &amp; Data Storage"},{"location":"using/getstarted/#home-directories","text":"Home directories are a small, fixed amount of storage per account. They are kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. More info...","title":"Home Directories"},{"location":"using/getstarted/#long-term-storage","text":"LTS is used to store raw and \"finished\" project data. The LTS directories are not available on the cluster nodes. It is kept on fault-tolerant storage with snapshops. More info...","title":"Long Term Storage"},{"location":"using/getstarted/#high-throughput-storage","text":"High Throughput Storage is a distrubuted file system able to handle tens of GBs/sec of total throughput. More info...","title":"High Throughput Storage"},{"location":"using/getstarted/#gui-software","text":"Note As the HTCF is primarily a batch queuing system for high-throughput processing of data, use of GUI applications are not directly supported. While use of GUI applications is possible using X forwarding, this can sometimes require significant desktop preparation and configuration which is beyond the scope of support.","title":"GUI Software"},{"location":"using/getstarted/#jobs","text":"","title":"Jobs"},{"location":"using/getstarted/#resources","text":"The number of CPUs and MBs of RAM per node can be found using the Slurm sinfo command: $ sinfo -N -p general -o '%n %c %m'","title":"Resources"},{"location":"using/getstarted/#interactive","text":"Interactive sessions are for running interactive scripts, vizualization, any tasks that are too computational intensive to run on the login node not submitted via sbatch. The defaults are: 1 CPU core, 1 GB RAM, and a time limit of 8 hours. Note The HTCF is primarily a batch queuing system. Interactive jobs are meant to function as daily workspaces. Because interactive jobs are by their nature, inefficient, they are not meant to be running continuously for more than 1 day. When using interactive tools such as rstudio or jupyter, please make sure the jobs are using the \"interactive\" queue (using sbatch/srun parameters -J interactive -p interactive ) Jobs using interactive tools that are not in the interactive queue will be subject to cancellation in order to free up resources for batch jobs. Tools such as Rscript can be used to run R programs in a batch fashion. It appears that jupyter notebooks can also be run in a batch fashion . Thanks for helping to ensure fairness for all folks on the HTCF. An interactive session can be started using the Slurm srun command: $ srun --mem-per-cpu=<MBs> --cpus-per-task=<num> -J interactive -p interactive --pty /bin/bash -l","title":"Interactive"},{"location":"using/getstarted/#batch-job-submission","text":"Determine resources Create Job File Create sbatch file with required resources Submit Monitor","title":"Batch Job Submission"},{"location":"using/getstarted/#workflow","text":"Jobs typically follow a generic workflow. Preprocessed Raw Data Enters LTS Raw Data is copied to scratch for processing Post processed data is copied to LTS Intermediate data generated in Step 2 is removed","title":"Workflow"},{"location":"using/getstarted/#sbatch-examples","text":"Create a job script (myjob.sbatch): #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem=1G eval $( spack load --sh <program> ) program /scratch/lab/files/ABC.fasta /scratch/lab/files/ABC.out Submit the sbatch script: $ sbatch myjob.sbatch View the job in the queue: $ squeue","title":"Sbatch Examples"},{"location":"using/getstarted/#gpus","text":"The HTCF currently has a small number of GPUs, currently 6 NVIDIA A100 80GB and 2 V100 32GB. A GPU is accessible using the following slurm parameters: -p gpu --gpus=<num> In an sbatch: #SBATCH -p gpu #SBATCH --gpus=<num> You can verify the GPU is being utilized by the job with the nvidia-smi command, this example runs within your job allocation: $ srun --ntasks-per-node=1 --jobid={jobid} nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 525.60.13 Driver Version: 525.60.13 CUDA Version: 12.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA A100 80G... Off | 00000000:17:00.0 Off | 0 | | N/A 39C P0 62W / 300W | 0MiB / 81920MiB | 23% Default | | | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 12043 C guppy_basecaller 4753MiB | +-----------------------------------------------------------------------------+","title":"GPUs"},{"location":"using/interactive/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Interactive"},{"location":"using/queue/","text":"Queuing System - Slurm The HTCF utlizes the Simple Linux Utility for Resource Management (Slurm). Slurm documentation can be found at http://slurm.schedmd.com/documentation.html . Job Submission There are two type of Slurm jobs, batch and interactive. Batch Jobs The steps needed to submit batch jobs are: Create a \"job script\". This is the file that actually does the work of the job. Create a \"sbatch script\". This file sets the Slurm parameters and prepares the environment for the job script. Launch the job using the \"sbatch\" command salloc - Obtains a job allocation. --cpu-per-task - Number of CPUs required per task --dependency=<state:jobid> --job-name=<name> --mem=<MB> Memory required per node. --mem-per-cpu=<MB> Memory required per allocated CPU. sbatch - Submits batch scripts for execution. srun - Obtains job allocation and executes an application. Partitions Partition Max Memory Duration Max CPUs in Queue general 250GB no limit 3004 interactive 250GB 8 hours 3004 Job Management squeue To view your job status in the queue scancel Users can use scancel command to cancel their jobs or job arrays. You may see job states of CA or CG during this processes. scancel JOBID scancel -u $USER Job Accounting sacct is the command to view all previously run job information. You can get a list of viewable fields by running the command sacct -e To view a past jobs maximum used memory and duration sacct -j JOBID --format=JobID,JobName,MaxRSS,Elapsed Scontrol can be used to view detailed information about your running job including the job script that was submitted. Please send the output of this command if your currently running job is having issues. ~$ scontrol show jobid -dd 846115 JobId=846115 JobName=sleep.sh UserId=ericmartin(1002) GroupId=ericmartin(1002) Priority=3070 Nice=0 Account=htcfadmin QOS=normal JobState=RUNNING Reason=None Dependency=(null) Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 DerivedExitCode=0:0 RunTime=00:00:09 TimeLimit=UNLIMITED TimeMin=N/A SubmitTime=2016-03-09T09:47:08 EligibleTime=2016-03-09T09:47:08 StartTime=2016-03-09T09:47:09 EndTime=Unknown PreemptTime=None SuspendTime=None SecsPreSuspend=0 Partition=general AllocNode:Sid=n082:21380 ReqNodeList=(null) ExcNodeList=(null) NodeList=n082 BatchHost=n082 NumNodes=1 NumCPUs=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:* Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* Nodes=n082 CPU_IDs=3-4 Mem=2000 MinCPUsNode=1 MinMemoryCPU=1000M MinTmpDiskNode=0 Features=(null) Gres=(null) Reservation=(null) Shared=OK Contiguous=0 Licenses=(null) Network=(null) Command=/scratch/htcfadmin/eric/sleep.sh WorkDir=/scratch/htcfadmin/eric StdErr=/scratch/htcfadmin/eric/slurm-846115.out StdIn=/dev/null StdOut=/scratch/htcfadmin/eric/slurm-846115.out BatchScript= #!/bin/bash #SBATCH -n 2 #SBATCH -N 1 module load bowtie2 sleep 100 More information on usage is available at http://slurm.schedmd.com/sacct.html . More information available here: * http://slurm.schedmd.com/overview.html * http://slurm.schedmd.com/tutorials.html * http://slurm.schedmd.com/faq.html","title":"Queue"},{"location":"using/queue/#queuing-system-slurm","text":"The HTCF utlizes the Simple Linux Utility for Resource Management (Slurm). Slurm documentation can be found at http://slurm.schedmd.com/documentation.html .","title":"Queuing System - Slurm"},{"location":"using/queue/#job-submission","text":"There are two type of Slurm jobs, batch and interactive.","title":"Job Submission"},{"location":"using/queue/#batch-jobs","text":"The steps needed to submit batch jobs are: Create a \"job script\". This is the file that actually does the work of the job. Create a \"sbatch script\". This file sets the Slurm parameters and prepares the environment for the job script. Launch the job using the \"sbatch\" command salloc - Obtains a job allocation. --cpu-per-task - Number of CPUs required per task --dependency=<state:jobid> --job-name=<name> --mem=<MB> Memory required per node. --mem-per-cpu=<MB> Memory required per allocated CPU. sbatch - Submits batch scripts for execution. srun - Obtains job allocation and executes an application.","title":"Batch Jobs"},{"location":"using/queue/#partitions","text":"Partition Max Memory Duration Max CPUs in Queue general 250GB no limit 3004 interactive 250GB 8 hours 3004","title":"Partitions"},{"location":"using/queue/#job-management","text":"","title":"Job Management"},{"location":"using/queue/#squeue","text":"To view your job status in the queue","title":"squeue"},{"location":"using/queue/#scancel","text":"Users can use scancel command to cancel their jobs or job arrays. You may see job states of CA or CG during this processes. scancel JOBID scancel -u $USER","title":"scancel"},{"location":"using/queue/#job-accounting","text":"sacct is the command to view all previously run job information. You can get a list of viewable fields by running the command sacct -e To view a past jobs maximum used memory and duration sacct -j JOBID --format=JobID,JobName,MaxRSS,Elapsed Scontrol can be used to view detailed information about your running job including the job script that was submitted. Please send the output of this command if your currently running job is having issues. ~$ scontrol show jobid -dd 846115 JobId=846115 JobName=sleep.sh UserId=ericmartin(1002) GroupId=ericmartin(1002) Priority=3070 Nice=0 Account=htcfadmin QOS=normal JobState=RUNNING Reason=None Dependency=(null) Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 DerivedExitCode=0:0 RunTime=00:00:09 TimeLimit=UNLIMITED TimeMin=N/A SubmitTime=2016-03-09T09:47:08 EligibleTime=2016-03-09T09:47:08 StartTime=2016-03-09T09:47:09 EndTime=Unknown PreemptTime=None SuspendTime=None SecsPreSuspend=0 Partition=general AllocNode:Sid=n082:21380 ReqNodeList=(null) ExcNodeList=(null) NodeList=n082 BatchHost=n082 NumNodes=1 NumCPUs=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:* Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* Nodes=n082 CPU_IDs=3-4 Mem=2000 MinCPUsNode=1 MinMemoryCPU=1000M MinTmpDiskNode=0 Features=(null) Gres=(null) Reservation=(null) Shared=OK Contiguous=0 Licenses=(null) Network=(null) Command=/scratch/htcfadmin/eric/sleep.sh WorkDir=/scratch/htcfadmin/eric StdErr=/scratch/htcfadmin/eric/slurm-846115.out StdIn=/dev/null StdOut=/scratch/htcfadmin/eric/slurm-846115.out BatchScript= #!/bin/bash #SBATCH -n 2 #SBATCH -N 1 module load bowtie2 sleep 100 More information on usage is available at http://slurm.schedmd.com/sacct.html . More information available here: * http://slurm.schedmd.com/overview.html * http://slurm.schedmd.com/tutorials.html * http://slurm.schedmd.com/faq.html","title":"Job Accounting"}]}