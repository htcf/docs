{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"High Throughput Computing Facility @ CGS_SB","text":"<p>The Center for Genome Sciences and Systems Biology is the proud home of The High Throughput Computing Facility, a Washington University recharge center.  The HTCF provides high-throughput computational resources for researchers within the CGS_SB.</p> <p>An HTCF description for grant writing purposes</p> <p>The CGSSB provides a computational cluster for high-throughput bioinformatics. The cluster consists of around 2000 processors and over 30TB of RAM.  It supports the Center's Illumina sequencing platforms and real time sequencing analysis. Long term data storage is handled by our 40 GbE connected storage arrays. These arrays are currently over 4 petabytes in size. All user data is backed up and stored daily, weekly and monthly. A disaster recovery copy of select data is stored offsite.</p> <p>The HTCF also includes a 750TB high-speed distributed file system that is capable of throughput up to 19GB/s. This, coupled with its 100Gb network backbone, delivering 10Gb of bandwidth to cluster nodes, the HTCF can provide exceptionally high-speed transfer of large amounts of data.</p>"},{"location":"policies/","title":"Policies","text":""},{"location":"policies/#wustl-computer-use","title":"WUSTL Computer Use","text":"<p>http://wustl.edu/policies/compolicy.html</p>"},{"location":"policies/#account-usage","title":"Account Usage","text":"<p>As stated in the above WUSTL Policy: \"Do not use the password of others or access files under false identity.\" Accounts and passwords cannot be shared. All users must have their own account.</p>"},{"location":"policies/#account-renewal","title":"Account Renewal","text":"<p>HTCF user accounts are automatically renewed annually from the original activation date unless otherwise instructed.</p>"},{"location":"policies/#account-removal","title":"Account Removal","text":"<p>Home directories of expired accounts are removed 90 days after expiration.</p>"},{"location":"policies/#storage-policies","title":"Storage Policies","text":""},{"location":"policies/#scratch-data-cleaning","title":"Scratch Data Cleaning","text":"<p>In order to ensure top performance of /scratch it is important to clean it regularly to remove stale data.  Therefore, the following weekly automated tasks are performed on /scratch:</p> <ul> <li>User files on scratch that have not been modified for more than 60 days are garbage collected and placed in a \u201ctrash\u201d location.</li> <li>After 30 days in the trash location, user files are purged from the system.  Once purged, there is no way files can be restored.</li> </ul> <p>Please ensure that any files you need for more than 60 days are safely copied to an LTS bucket.</p> <p>Garbage-collected files are stored in /scratch/trash/&lt;date_of_collection&gt;/.</p> <p>You can restore your garbage-collected files by moving them out of this directory.</p> <p>A list of your garbage-collected files can be found in /scratch/trash/&lt;date_of_collection&gt;/filelists/&lt;username&gt;.</p> <p>The HTCF is not responsible for data loss from automated scrubs.  Labs are responsible for monitoring their files and transferring their data from scratch to long term storage.</p>"},{"location":"policies/#data-limits","title":"Data Limits","text":"<p>Each member of the HTCF belongs to at least two Unix groups.  The primary group is your personal group, having the same name as your HTCF username.  The secondary group is the laboratory or similar entity that you are primarily associated with.</p> <p>Policy: Scratch user data limits</p> <ul> <li>Size Limit - 2TB</li> <li>Inode Limit (Number of files) - 2,000,000</li> </ul> <p>Example</p> <p>Username:  johnsmith</p> <p>To determine your personal scratch usage:</p> <pre><code>   $ beegfs-ctl --getquota --uid johnsmith\n</code></pre>"},{"location":"policies/#login-node-policy","title":"Login Node Policy","text":"<p>The HTCF login node is to be used for job composition, software installation, and staging of job data.  Any computational processes found running longer than 30 minutes can be terminated.  </p>"},{"location":"policies/#general-availability","title":"General Availability","text":"<p>Effort will be made to keep our resources available. Although the support personnel will do their best to keep the facility running at all times, we cannot guarantee to promptly resolve problems outside office hours, during weekends, and public holidays. Nevertheless, please notify us of whenever they arise.</p>"},{"location":"policies/#general-maintenance","title":"General Maintenance","text":"<p>Occasionally, it is necessary as part of maintaining a reliable service to update system software and replace faulty hardware. Sometimes it will be possible to perform these tasks transparently by means of queue reconfiguration in a way that will not disrupt running jobs or interactive use, or significantly inconvenience users. Some tasks however, particularly those affecting storage or login nodes, may require temporary interruption of service.</p>"},{"location":"policies/#running-jobs","title":"Running Jobs","text":"<ul> <li>Jobs that improperly perform excessive I/O, or utilize unreserved CPU time will be terminated. </li> <li>Please be accurate when requesting memory, not requesting enough memory will result in your process crashing, requesting too much memory will prevent other users from running jobs.</li> </ul>"},{"location":"prerequisites/","title":"Prerequisites","text":"<p>A firm grasp of the following concepts and technologies are expected for users of the HTCF:</p>"},{"location":"prerequisites/#bash-shell","title":"Bash Shell","text":"<p>https://www.gnu.org/software/bash/manual/html_node/index.html</p>"},{"location":"prerequisites/#bash-environment-variables","title":"Bash Environment Variables","text":"<p>https://linuxhint.com/bash-environment-variables/</p>"},{"location":"prerequisites/#python-virtual-environments","title":"Python Virtual Environments","text":"<p>https://docs.python.org/3/tutorial/venv.html https://docs.python.org/3/library/venv.html, https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/</p>"},{"location":"runningjobs/","title":"Runningjobs","text":"<p>Many times you might have a command that you\u2019d like to run on a lot of samples.  To run these sequentially, you might have a script like this:</p> <pre><code>#!/bin/bash\n\nmodule load spades\n\nspades.py --careful --pe1-1 samp1-r1.fastq --pe1-2 samp1-r2.fastq -o assembly_1\nspades.py --careful --pe1-1 samp2-r1.fastq --pe1-2 samp2-r2.fastq -o assembly_2\n\u2026\nspades.py --careful --pe1-1 samp2000-r1.fastq --pe1-2 samp2000-r2.fastq -o assembly_2000\n</code></pre> <p>By submitting this on the HTCF as an \u201carray job\u201d, you have the potential of running each of these commands in parallel (all at the same time) rather than sequentially (one at a time).  The best case scenario could be that all 2000 commands finish in the time it takes 1 command to run!  More information regarding SLURM job arrays is available at http://slurm.schedmd.com/job_array.html.</p>"},{"location":"runningjobs/#run-a-command-or-set-of-commands-on-file-names-that-are-sequentially-numbered","title":"Run a command or set of commands on file names that are sequentially numbered","text":"Input File 1 Input File 2 sample1-r1.fastq sample1-r2.fastq sample2-r1.fastq sample2-r2.fastq \u2026 \u2026 sample2000-r1.fastq sample2000-r2.fastq <p>Step 1:  Create an sbatch file</p> <pre><code>#!/bin/bash\n\n#SBATCH --array=1-2000%20   # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run\n\nmodule load spades\n\nID=${SLURM_ARRAY_TASK_ID}  # Number between 1 and 2000 \n\nspades.py --careful --pe1-1 samp${ID}-r1.fastq --pe1-2 samp${ID}-r2.fastq -o assembly_${ID}\n</code></pre>"},{"location":"runningjobs/#run-a-command-or-set-of-commands-on-file-names-that-arent-numbered-sequentially","title":"Run a command or set of commands on file names that aren\u2019t numbered sequentially","text":"Input File 1 Input File 2 sampleAAA-r1.fastq sampleAAA-r2.fastq sampleAAB-r1.fastq sampleAAB-r2.fastq \u2026 \u2026 SampleZZZ-r1.fastq SampleZZZ-r2.fastq <p>Step 1:  Create a \u201clookup\u201d file, lookup.txt <pre><code>AAA\nAAB\n\u2026\nZZZ\n</code></pre></p> <p>Step 2: Find the number of lines in lookup file.  This will be the number of tasks in your array job.</p> <p><pre><code>$ wc -l lookup.txt\n2000 lookup.txt\n</code></pre> Step 2: Create an sbatch file, grabbing the \u201cID\u201d in line $SLURM_ARRAY_TASK_ID from the lookup file <pre><code>#!/bin/bash\n\n#SBATCH --array=1-2000%20   # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run\n\nmodule load spades\n\nID=$( sed -n ${SLURM_ARRAY_TASK_ID}p lookup.txt )\n\nspades.py --careful --pe1-1 samp${ID}-r1.fastq --pe1-2 samp${ID}-r2.fastq -o assembly_${ID}\n</code></pre></p>"},{"location":"runningjobs/#run-a-command-or-set-of-commands-using-a-complex-lookup-file","title":"Run a command or set of commands using a complex lookup file","text":"<p>Step 1:  Create a \u201clookup\u201d file, lookup.txt <pre><code>5 sampleAAA_R1 0.0001 5000 \n5 sampleAAA_R2 0.0001 5000\n5 sampleBBB_R1 0.0005 1000 \n5 sampleBBB_R2 0.0005 1000 \n</code></pre></p> <p>Step 2:  Find the number of lines in lookup file.  This will be the number of tasks in your array job.</p> <p><pre><code>$ wc -l lookup.txt\n2000 lookup.txt\n</code></pre> Step 3: Create an sbatch file, each job is created using the template with information from the lookup file. <pre><code>#!/bin/bash\n\n#SBATCH --array=1-2000%20   # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run\n\nmodule load seqtk\n\nread part1 part2 part3 part4 &lt; &lt;( sed -n ${SLURM_ARRAY_TASK_ID}p lookup.txt )\n\nseqtk sample -s${part1} /path/to/file.${part2}.fastq ${part3} &gt; /path/to/file.${part2}_${part4}.fastq\n</code></pre></p>"},{"location":"software/","title":"Software","text":"<p>The HTCF starts as somewhat of a blank slate for each lab.  However, this doesn't mean labs can't get up and running in a matter of minutes!</p> <p>Each lab has its own dedicated space to install and manage software.  This reference space is located in <code>/ref/&lt;lab&gt;/software</code>.</p> <p>Software building and installation on the HTCF is primarily self-service.</p> <p>Labs are free to use their <code>/ref</code> software directory to install software using whatever means is most comfortable.</p> <p>At the lab level, use of Spack to install common software is encouraged.  Virtual environments can also be used if the software is well suited.</p>"},{"location":"software/#spack","title":"Spack","text":"<p>Spack is a package management tool designed to support multiple versions and configurations of software on a wide variety of platforms and environments.</p> <p>Note</p> <p>Do not install software while on the login node.  Please build/install software from an interactive job.</p>"},{"location":"software/#tutorial","title":"Tutorial","text":"<p>See the official spack tutorial</p>"},{"location":"software/#initialization","title":"Initialization","text":"<p>To create a lab instance of the spack package manager:</p> <ol> <li> <p>Download and untar a spack release into <code>/ref/&lt;lab_name&gt;/software</code></p> </li> <li> <p>Rename (or make a symlink from) <code>/ref/&lt;lab_name&gt;/software/spack-VERSION</code> to <code>/ref/&lt;lab_name&gt;/software/spack</code>.</p> </li> <li> <p>Logout and log back in.  This will ensure the spack command is available in the PATH.</p> </li> </ol>"},{"location":"software/#installing-software","title":"Installing Software","text":"<p>see the official spack documentation</p> <p>Warning</p> <p>Please install software from within a Slurm job.</p> <p>Note</p> <p>Some compiling requires large amounts of RAM.  Using <code>--mem-per-cpu</code> with &gt;= 4G is sometimes needed.</p> <p>In the case of software that requires the \"qt\" package, more than --mem-per-cpu=10G could be needed.</p> <p>Note</p> <p>When installing within a slurm job, be sure to tell Spack how many CPUs are available.</p> <p>For example, after getting an interactive session:</p> <pre><code>spack install -j ${SLURM_CPUS_ON_NODE} .....\n</code></pre> <p>Note</p> <p>Some software can take an extremely long time to install (such as qt and llvm).  In these cases, an sbatch job will be needed rather than an interactive job:</p> <pre><code>#!/bin/bash\n#SBATCH -c &lt;num&gt;\n#SBATCH --mem-per-cpu=&lt;num&gt;G\nspack install -j ${SLURM_CPUS_ON_NODE} ....\n</code></pre>"},{"location":"software/#using-the-software","title":"Using the software","text":"<p>Once spack has built software, the bash shell needs to have the proper environment variables set to access the software.</p> <p>This is accomplished using the <code>spack load</code> command.  Spack packages can be \"loaded\" similar to the way modules are loaded.</p> <p>Given a spec, a spack command can be used to generate the appropriate environment variables to \"load\" spack-installed software.</p> <p>To set the environment variables (similar to <code>module load ...</code>):</p> <pre><code>$ eval $( spack load --sh &lt;spec&gt; )\n</code></pre> <p>To unset (unload) these variables:</p> <pre><code>$ eval $( spack unload --sh &lt;spec&gt; )\n</code></pre> <p>To simply view the environment variables that would be set without actually setting them:</p> <pre><code>$ spack load --sh &lt;spec&gt;\n</code></pre> <p>See the official spack documentation for more information on specs.</p> <p>These commands can be placed in an sbatch file to be used in a job.</p> <pre><code>#!/bin/bash\n\neval $( spack load --sh &lt;spec&gt; )\n</code></pre>"},{"location":"software/#example","title":"Example","text":"<p>Installing biom-format</p> <p>Search for the name of the package:</p> <pre><code>$ spack list biom\n==&gt; 7 packages.\nmicrobiomeutil  py-biomine    r-biomart   r-biomformat\npy-biom-format  r-biom-utils  r-biomartr\n</code></pre> <p>See what versions are available:</p> <pre><code>$ spack versions py-biom-format\n==&gt; Safe versions (already checksummed):\n  2.1.10  2.1.9  2.1.7  2.1.6\n...\n</code></pre> <p>Install:</p> <pre><code>$ spack install py-biom-format@2.1.10\n</code></pre> <p>Load the software in a job:</p> <pre><code>#!/bin/bash\n\neval $( spack load --sh py-biom-format@2.1.10 )\n\n...\n</code></pre>"},{"location":"software/#what-if-the-software-i-want-is-not-available-through-spack","title":"What if the software I want is not available through Spack","text":"<p>When needed software is not readily accessible via Spack, there are a few options.</p> <ol> <li> <p>Follow the installation instructions from the software creator.</p> <p>Sometimes, this can be very quick and straightforward.</p> <p>Sometimes, this can be very painful.</p> <p>Sometimes, it can be a good idea to pass judgement on the quality of software based on the quality of the installation process and documentation. </p> </li> <li> <p>Create a custom spack package</p> <p>Spack can be a wonderful tool for creating and maintaining software. Plenty of documentation is provided for creating and maintaining custom packages, though a firm understanding of python is needed.</p> </li> </ol>"},{"location":"software/#r-considerations","title":"R considerations","text":"<p>Please see the R page for more information.</p>"},{"location":"software/#manual-installation","title":"Manual Installation","text":"<p>Sometimes it's just easier to follow the installation steps provided by software creator.</p> <p>If the software depends on other software, it might be that the dependency software could be installed via spack.</p> <p>For example, if a piece of software is not available via Spack, but requires samtools to be installed:</p> <pre><code># Install samtools via spack\n$ spack install samtools\n\n# load samtools before installation\n$ eval $( spack load --sh samtools )\n\n(proceed with the manual installation)\n</code></pre> <p>Remember</p> <p>The best place to install software is in reference storage: <code>/ref/&lt;lab&gt;/software</code>.</p> <p>After manual software installation, it's good practice to then create a module file</p>"},{"location":"software/#manual-module-files","title":"Manual module files","text":"<p>Module files that are manually created go in reference storage in <code>/ref/&lt;lab&gt;/software/modules</code>.</p> <p>The lmod documentation is the best place to learn about creating module files.</p>"},{"location":"software/#what-about-conda","title":"What about conda?","text":"<p>Feel free to use conda if it is required/preferred.</p> <p>The HTCF does not handle Conda support requests.  For conda support, please see https://docs.conda.io/en/latest/help-support.html</p>"},{"location":"software_old/","title":"Software old","text":""},{"location":"software_old/#modules","title":"Modules","text":"<p>Lmod is a Lua based module system that easily handles the MODULEPATH Hierarchical problem. Environment Modules provide a convenient way to dynamically change the users' environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found.</p> <p>Software is handled using lmod.  There should be minimal need to modify your .bashrc or .profile unless you're installing software locally to test.</p>"},{"location":"software_old/#basics","title":"Basics","text":"<p>Lmod is managed using the command module, using this command without options will show you a list of all available subcommands.</p> <pre><code>~$ module\n\nModules based on Lua: Version 6.6  2016-10-13 13:28 -05:00\n    by Robert McLay mclay@tacc.utexas.edu\n\nmodule [options] sub-command [args ...]\n\nHelp sub-commands:\n------------------\n  help                              prints this message\n  help                module [...]  print help message from module(s)\n</code></pre> <p>A list of software available the command:</p> <pre><code>~$ module avail\n---------------------------------------------- /opt/apps/modules ----------------------------------------------\n   GD/2.1.1                            genometools/1.5.8                  pindel/0.2.5b6\n   R/2.15.3                            ghostscript/9.19                   pmap/11-25-2010\n   R/3.1.2                             glimmer/3.02b                      poretools/0.6.0\n</code></pre> <p>To load the latest (default) version of module:</p> <pre><code>module load ncbi-blast\n</code></pre> <p>To specifiy which version of the module you would like to use:</p> <pre><code>module load ncbi-blast/2.2.30+\n</code></pre> <p>Be sure to specify which version of the sofware you'd like to use in your scripts to ensure consistent results, as software updates may break pipelines.</p> <p>Software with prerequisites are loaded dynamically, for example:</p> <pre><code>~$ module load prokka\n~$ ml\n\nCurrently Loaded Modules:\n  1) aragorn/1.2.36   3) prodigal/2.6.2       5) tbl2asn/24.2       7) hmmer/3.1b1   9) prokka/1.11\n  2) infernal/1.1.1   4) ncbi-blast/2.2.31+   6) bio-perl/1.6.923   8) barrnap/0.6\n</code></pre>"},{"location":"software_old/#gui-software","title":"GUI Software","text":"<p>As the HTCF is primarily a batch queuing system for high-throughput processing of large amounts of data,  GUI application are not directly supported by the HTCF.  GUI application installation and setup on the HTCF are left to the end user.</p>"},{"location":"software/examples/r_with_mpi/","title":"Example: R+MPI","text":"<p>The following is an example of preparing for and running a job consisting of an R script that requires:</p> <ul> <li>mpi (<code>Rmpi</code>) to parallalize the work</li> <li>bioinformatics packages <code>sva</code> and <code>edgeR</code></li> </ul>"},{"location":"software/examples/r_with_mpi/#install-the-spack-managed-software-r-mpich-rmpi","title":"Install the Spack managed software (R, mpich, Rmpi)","text":"<p>Get an interactive session with plenty of resources to help install software quickly:</p> <pre><code>srun --mem-per-cpu=4G -c 8 -J interactive -p interactive --pty /bin/bash -l\n</code></pre> <p>Install (but don't load) software that will be managed by Spack (specifying exact versions if desired):</p> <pre><code># R\nspack install r@&lt;version&gt;\n\n# mpich\nspack install mpich\n\n# Rmpi\nspack install r-rmpi ^r@&lt;version&gt; ^mpich\n</code></pre> <p>Note</p> <p>These 3 separate commands aren't actually needed.  The same could be accomplished with just 1 command:</p> <pre><code>spack install r-rmpi ^r@&lt;version&gt; ^mpich\n</code></pre> <p>With 3 separate commands, Spack considers all three \"explicitly\" installed.</p> <p>With 1 command, only r-rmpi is considered \"explicity\" installed.  The rest are \"implicitly\" installed.</p> <p>See here for more information.</p> <p>Install libpng (which seems to be needed to install the <code>sva</code> R package later on)</p> <pre><code>spack install libpng\n</code></pre>"},{"location":"software/examples/r_with_mpi/#install-r-libraries-sva-and-edger","title":"Install R libraries (sva and edgeR)","text":"<p>Get an interactive session (if not already in one) with plenty of resources to help install software quickly:</p> <pre><code>srun --mem-per-cpu=4G -c 8 -J interactive -p interactive --pty /bin/bash -l\n</code></pre> <p>Ensure a clean environment (ie. unload any spack packages and/or modules that might be loaded from previous work):</p> <pre><code>eval $(spack unload --sh --all)\n</code></pre> <p>Load R (specifying version if desired):</p> <pre><code>eval $(spack load --sh r@&lt;version&gt;)\n</code></pre> <p>The <code>sva</code> R library seems to need <code>libpng</code> to be loaded:</p> <pre><code>eval $(spack load --sh libpng)\n</code></pre> <p>Decide on a shared location and name for the software and create the directory</p> <p>For example: </p> <pre><code>mkdir /ref/jdlab/software/projecta\n</code></pre> <p>Using an R environment variable, tell R where that location is so it will install packages there:</p> <pre><code>export R_LIBS_SITE=/ref/jdlab/software/projecta\n</code></pre> <p>Important</p> <ul> <li>This <code>R_LIBS_SITE</code> environment variable needs to be set any time R packages are installed for the project.</li> <li>This <code>R_LIBS_SITE</code> environment variable needs to be set any time the R packages need to be loaded in a job.</li> </ul> <p>Install <code>sva</code> and <code>edgeR</code> into <code>R_LIBS_SITE</code>:</p> <pre><code>$ R\n&gt; install.packages(\"BiocManager\")\n...\n...\n...\n&gt; BiocManager::install(\"edgeR\")\n...\n...\n...\n&gt; BiocManager::install(\"sva\")\n...\n...\n...\n</code></pre> <p>Put it all together in a job</p> <p>In the <code>.sbatch</code> job:     #!/bin/bash</p> <pre><code>#SBATCH -n X\n#SBATCH --mem-per-cpu=XG\n#SBATCH --cpus-per-task=X\n\neval $(spack load --sh r-rmpi ^r@&lt;version&gt; ^mpich)\nexport R_LIBS_SITE=/ref/jdlab/software/projecta\n\nmpiexec -usize $SLURM_NTASKS -np 1 Rscript /path/to/rscript.R\n</code></pre>"},{"location":"software/examples/r_mpi/","title":"Example: Using MPI with slurm via R and Rmpi","text":"<p>Getting all the necessary software properly built, installed, and configured can be very difficult.</p> <p>This combination of software/versions seems to work:</p> <ul> <li>Spack 0.18.0</li> <li>Slurm 20.11.9.1</li> <li>openmpi 4.1.3</li> <li>R 4.1.3</li> <li>r-rmpi 0.6-9.2</li> </ul>"},{"location":"software/examples/r_mpi/#installing","title":"Installing","text":"<p>Step 1. Get and interactive session</p> <p>Step 2. Remove Slurm variables from the interactive session's environment (or Rmpi will break when it tries to install):</p> <pre><code>$ for x in $(env|grep ^SLURM|cut -f1 -d=); do unset $x;done\n</code></pre> <p>Step 3. Install r-rmpi (with a very long spec):</p> <pre><code>$ spack install r-rmpi@0.6-9.2 ^r@4.1.3 ^openmpi@4.1.3 schedulers=slurm legacylaunchers=true ^slurm@20-11-9-1\n</code></pre>"},{"location":"software/examples/r_mpi/#using","title":"Using","text":"<p>Example sbatch file and R script.</p>"},{"location":"software/jupyter/","title":"Jupyter Lab","text":"<p>Jupyter Lab can be installed via Spack:</p> <pre><code>spack install py-jupyterlab\n</code></pre> <p>Example sbatch script:</p> <pre><code>#!/bin/bash\n\n# ------ SLURM Parameters ------\n\n# rstudio-server is interactive.  Use the interactive partition\n# Add cpu or memory slurm parameters as needed\n\n#SBATCH -p interactive\n\n# ------ Make sure it's run as a job ------\n\nif [ -z \"${SLURM_JOBID}\" ]; then\n    echo \"Error: Must be run as a job\"\n    exit 1\nfi\n\n# ------ Load environment ------\n\neval $(spack load --sh py-jupyterlab)\n\nport=$(shuf -i9000-9999 -n1)\n\necho -e \"\n\u00a0\u00a0\u00a0 In a local terminal, create SSH tunnel to $HOSTNAME\n\u00a0\u00a0\u00a0 -----------------------------------------------------------------\n\u00a0\u00a0\u00a0 ssh $USER@login.htcf.wustl.edu -N -L $port:$HOSTNAME:$port\n\u00a0\u00a0\u00a0 -----------------------------------------------------------------\n\n    Then in the desktop browser, follow the http://127.0.0.1..... address shown at the bottom of the jupyter lab command\n    \"\n\n# Launch jupyter lab\njupyter lab --no-browser --port=$port --ip=$HOSTNAME\n</code></pre>"},{"location":"software/r/","title":"R","text":"<p>It's best to install R using Spack, then after setting up the R search path, install R packages using the R command line interface, following the packages' install instructions.</p> <p>This is the best way to ensure the proper installation of the latest (or most appropriate) versions of the R libraries.</p>"},{"location":"software/r/#support","title":"Support","text":"<p>Reminder</p> <p>Beyond basic R installation and basic R package installation, the HTCF cannot handle R support requests.</p> <p>After installation, support requests for R software and packages should be directed to:</p> <ul> <li>Google</li> <li>Software support forums or mailing lists<ul> <li>https://www.r-project.org/help.html</li> <li>https://www.biostars.org/</li> </ul> </li> <li>The software developers</li> </ul>"},{"location":"software/r/#search-paths","title":"Search paths","text":"<p>Warning</p> <p>** A thorough understanding of R search paths is necessary for successful installation and use of R packages.**</p> <p>It's best to store R packages in a shared location (such as <code>/ref/&lt;lab&gt;/software/...</code>) rather than <code>$HOME/R</code>.  R packages in <code>$HOME</code> aren't as easily shared with other lab members and can quickly fill up the space in <code>$HOME</code>.</p> <p>The shared location must exist (<code>mkdir &lt;dir&gt;</code>) prior to starting the R command line.</p> <p>Please read and understand libPaths: Search Paths for Packages, specifically R_LIBS_USER and R_LIBS_SITE, before using R.  It may be best to include the R version in the path such as <code>/ref/&lt;lab&gt;/software/r_packages/%v</code></p> <p>For example, to set up a shared directory of R (version 4.1.X) packages and install into it:</p> <pre><code>$ mkdir -p /ref/&lt;labname&gt;/software/r-envs/&lt;project_name&gt;/4.1\n$ export R_LIBS_SITE=/ref/&lt;labname&gt;/software/r-envs/&lt;project_name&gt;/%v\n$ eval $( spack load --sh r@4.1.1 )\n$ R\n...\n&gt; install.packages('&lt;pkgname&gt;')\n</code></pre> <p>To use these R libraries in an Rscript job:</p> <pre><code>#!/bin/bash\n\nexport R_LIBS_SITE=/ref/&lt;labname&gt;/software/r-envs/&lt;project_name&gt;/%v\neval $( spack load --sh r@4.1.1 )\n\nRscript ........\n</code></pre>"},{"location":"software/r/#troubleshooting-package-installation","title":"Troubleshooting Package Installation","text":"<p>Unfortunately, trial and error may be required when installing R packages.</p>"},{"location":"software/r/#environment-variables","title":"Environment variables","text":"<p>It seems some R packages such as <code>Rsamtools</code> and <code>Rhtslib</code> require extra environment variables in order to find their dependencies during install.</p> <p>To do this, prior to installation, one or more of the following variables might need to be set:</p> <pre><code>export LIBRARY_PATH=$SPACK_LIBRARY_PATH\nexport C_INCLUDE_PATH=$SPACK_C_INCLUDE_PATH\nexport CPLUS_INCLUDE_PATH=$SPACK_CPLUS_INCLUDE_PATH\nexport LD_LIBRARY_PATH=$SPACK_LD_LIBRARY_PATH\n</code></pre> <p>This should help install some R packages.</p> <p>Warning</p> <p>Most R packages DO NOT need these environment variables, and they might actually FAIL to install properly if these variables are set.</p>"},{"location":"software/r/#installing-within-rstudio","title":"Installing within rstudio","text":"<p>Warning</p> <p>Some have reported problems installing software from within rstudio.</p> <p>It may be best to always install software from command-line R (from within an interactive job).</p>"},{"location":"software/rstudio/","title":"RStudio","text":"<p>Rstudio Server can be run as an interactive job and accessed via an SSH tunnel.</p> <p>Warning</p> <p>The Spack <code>rstudio</code> package is the Destop Version of rstudio and is NOT for use on the HTCF.</p>"},{"location":"software/rstudio/#support","title":"Support","text":"<p>Reminder</p> <p>The HTCF does not handle Rstudio support requests</p> <p>After installation, support requests for lab-installed software such as rstudio should be directed to:</p> <ul> <li>Google</li> <li>Software support forums or mailing lists</li> <li>The software developers</li> </ul> <p>Note</p> <p>The <code>texlive</code> package may need modifying before installing rstudio-server. If there is a texlive error during install, the following two lines should be commented out in the texlive package (<code>spack edit texlive</code>):</p> <pre><code>#    version('live', sha256='74eac0855e1e40c8db4f28b24ef354bd7263c1f76031bdc02b52156b572b7a1d',\n#        url='ftp://tug.org/historic/systems/texlive/2021/install-tl-unx.tar.gz')\n</code></pre>"},{"location":"software/rstudio/#building-and-installing","title":"Building and Installing","text":"<p>Warning</p> <p>Compiling rstudo requires large amounts of RAM.  When setting up the Slurm jobs be sure to include:</p> <pre><code>--mem-per-cpu=4G --cpus-per-task=&lt;NUM&gt;\n</code></pre> <p>in the Slurm parameters. (More than 1 CPU will make the building faster but could cause longer waiting in the queue.)</p> <ol> <li> <p>A custom Spack package needs to be created: <code>spack create rstudio-server</code></p> </li> <li> <p>An example rstudio-server package.py (tested against Spack-0.18.0) can be found here. (Older Spack-0.17.2 version here)</p> </li> </ol> <p>Note</p> <p>Be sure to tell Spack how many CPUs are available:</p> <pre><code>spack install -j ${SLURM_CPUS_PER_TASK} rstudio-server\n</code></pre>"},{"location":"software/rstudio/#running-rstudio-server-as-a-job","title":"Running rstudio-server as a job","text":"<p>A sample <code>rstudio.sbatch</code> script can be found here.</p> <p>Note</p> <p>The rstudio.sbatch script will likely need to be modified to add CPU and/or Memory requirements.</p>"},{"location":"storage/","title":"Storage","text":"<p>The HTCF provides five types of storage:</p>"},{"location":"storage/#hds","title":"HDS","text":"<p>Home Directory Storage (HDS) can be used to store scripts, development tools, etc.  Home directories are located in <code>/home/&lt;WUSTLKEY_ID&gt;</code> and are available on all nodes. Home directory space is limited to 20GB.</p> <p>HDS is kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss.  Copies of the latest daily snapshots are kept offsite for disaster recovery purposes.</p> <p>Note</p> <p>Home directory space is not a high-speed resource like /scratch space.</p> <p>Please keep home directory access to a minimum in batch jobs.</p> <p>By default, home directories will not be readable or writeable by other users of HTCF.  Feel free to change this default, if desired.</p> <p><code>/home/usage.txt</code> contains home directory usage:</p> <pre><code>$ grep $USER /home/usage.txt\n</code></pre>"},{"location":"storage/#lts","title":"LTS","text":"<p>Long Term Storage (LTS) is lab project space to store raw sequencing and completed data, the directories are not available on nodes for computational use.  It is available in terabyte increments billed monthly.  It is kept on fault-tolerant storage with snapshops.  Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. </p> <p>To check LTS usage:</p> <pre><code>$ df -h /lts/&lt;lab_name&gt;/&lt;bucket_name&gt;\n</code></pre>"},{"location":"storage/#ltos-long-term-object-storage","title":"LTOS - Long Term Object Storage","text":"<p>LTOS is an architecture that manages data as objects, as opposed to traditional LTS which uses file systems and block storage.</p> <p>LTOS is located onsite and is not Amazon S3, but is a subset, and works in a similar way as Amazon S3. </p> <p>Unlike LTS, which is only accessible from the login server, LTOS is accessible externally and from all HTCF nodes.</p>"},{"location":"storage/#purpose","title":"Purpose","text":"<p>LTOS can be a good alternative to LTS any time the data in question doesn't need to be heavily manipulated or modified.</p> <p>Good Candidates for LTOS:</p> <ul> <li>Raw sequence data and finished analysis data</li> <li>Archived or rarely referenced data such as alumni files.</li> <li>Data that is not often modified, once created.</li> </ul>"},{"location":"storage/#using","title":"Using","text":"<p>When purchasing LTOS, please indicate whether the storage needs to be backed up offsite or only one copy is needed onsite. The per-TB cost of LTOS with an offsite copy is the same as LTS.  If offsite storage is not needed, the cost is about 1/3.</p> <p>An \"access key\" and \"secret key\" will be assigned and can be used to connect to the storage. Buckets can be created and data can be transferred in/out using a command line s3 transfer tool.  The s3cmd tool is recommended. It's available via spack as \"py-s3cmd\".</p> <p>After s3cmd is installed, a config file needs to be created.</p> <p>For LTOS with an offsite copy:</p> <pre><code>cat &gt; s3cmd-mystorage.conf &lt;&lt;EOF\n[default]\nhost_base = s3-obs2.htcf.wustl.edu\nhost_bucket = s3-obs2.htcf.wustl.edu\naccess_key = ...\nsecret_key = ...\nEOF\n</code></pre> <p>For LTOS with no offsite copy:</p> <pre><code>cat &gt; s3cmd-mystorage.conf &lt;&lt;EOF\n[default]\nhost_base = s3-obs1.htcf.wustl.edu\nhost_bucket = s3-obs1.htcf.wustl.edu\naccess_key = ...\nsecret_key = ...\nEOF\n</code></pre> <p>s3cmd can reference the config via a parameter:</p> <pre><code>s3cmd -c s3cmd-mystorage.conf &lt;cmd&gt;\n</code></pre> <p>To integrate LTOS access into software, there are many s3 API libraries for various programming languages.</p>"},{"location":"storage/#ref","title":"REF","text":"<p><code>/ref</code> is storage space for software and reference databases (such as NCBI databases or software-provided reference sequences).  Each lab has an initial 1TB of reference space, and this space can be expanded at LTS prices.</p> <p>Note</p> <p><code>/ref</code> is not currently backed up, therefore Any data in <code>/ref</code> that cannot be recreated should be copied to long term storage (LTS or LTOS) for safe-keeping.</p> <pre><code>/ref\n\u251c\u2500\u2500 &lt;lab&gt;\n    \u251c\u2500\u2500 data\n    \u2514\u2500\u2500 software\n</code></pre> <p>The <code>data</code> directory is well suited for modestly sized reference data such as NCBI blast databases. Larger datasets (&gt; 500GB) are probably better suited for Long Term Object Storage</p> <p>The <code>software</code> directory is suited for software installation using tools such as spack</p>"},{"location":"storage/#hts","title":"HTS","text":"<p>High Throughput Storage (<code>/scratch</code>) is a distrubuted file system able to handle tens of GBs/sec of total throughput.  This storage is temporary scratch space and is not backed up.  Once data is removed from /scratch, it cannot be recovered.</p> <p>Data stored in /scratch is subject to the Scratch Data Cleaning Policy.</p> <p>Jobs utilize this space for inputs and outputs to provide the best performance possible.  Running jobs that read/write from the home directory will cause slowness and login issues for all users.</p> <p>Note</p> <p>Users will be asked to clean up older files on /scratch if it is needed to improve system performance.</p> <p>The best use of the HTS is to use a workflow similar to the following:</p> <ol> <li>Copy starting (raw) data from LTS to HTS.</li> <li>Submit jobs to the cluster that process the data, creating intermediate and/or finished data.</li> <li>Copy the finished data (and job files used to create that data) over to LTS.</li> <li>Remove all working data from HTS</li> </ol> <p>Results that are generated on this storage need to be promptly copied to LTS. </p>"},{"location":"storage/#scratch-quotas","title":"Scratch Quotas","text":"<p>There is a quota of 2TB per user in /scratch to prevent the filesystem from filling up.  At &gt;85% /scratch can become very slow.  To check the amount of space being used, use the following command:</p> <pre><code>$ beegfs-ctl --getquota --uid $USER\n</code></pre> <p>or check group quotas with:</p> <pre><code>$ beegfs-ctl --getquota --gid GROUPNAME\n</code></pre>"},{"location":"storage/#quota-increase-requests","title":"Quota Increase Requests","text":"<p>Note</p> <p>A quota increase is not guaranteed.  If excess capacity is not available, a quota increase cannot be granted.</p> <p>If excess capacity is available.  A temporary increase in the scratch quota can be requested.  To request more scratch space, email following information:</p> <ol> <li>Reason for the increase</li> <li>Amount of additional space</li> <li>Duration additional space will be required</li> </ol>"},{"location":"storage/#recommendations","title":"Recommendations","text":"<ul> <li>No important source code, scripts, libraries, executables should be kept in <code>/scratch</code></li> <li>Do not make symlinks from the home directory to folders in <code>/scratch</code></li> </ul>"},{"location":"storage/#sharing-files-publicly","title":"Sharing Files Publicly","text":""},{"location":"storage/#globus","title":"Globus","text":"<p>Globus can be used to share data from LTS, /scratch, or Home directories.  The Globus \"Collection\" is called \"HTCF@WUSTL\". From here, selected directories can be shared as \"Guest Collections\".  Please see the globus documentation for more information</p> <p>Note</p> <p>In order to share a directory, it (and all its parent directories) need to be readable by all users.</p> <p>For long term hosting of publicly accessible data, please contact WUSTL IT.</p>"},{"location":"storage/#copying-files-using-rsync","title":"Copying Files Using Rsync","text":"<p>Using rsync to transfer to scratch and LTS is recommended.  Rsync can resume failed copies, be re-run to ensure all of the data has been transferred, and will also transfer incremental changes.  This will save a substantial amount of time if it is necessary to verify that all files have been successfully copied.</p> <p>When using this command, please note that the absense of a trailing slash means the directory, with a trailing slash means the contents of that directory.  Here are a few examples:</p> <pre><code>~$ rsync -aHv --progress /directory/to/transfer /destination/directory/location/\n</code></pre> <p>The above example would put the directory named \"transfer\" into the directory named \"location\"</p> <pre><code>~$ rsync -aHv --progress /directory/to/transfer/ /destination/directory/location/\n</code></pre> <p>The above example would put the contents of the directory named \"transfer\" into the directory named \"location\". More info...</p>"},{"location":"storage/#disk-quota-exceeded-errors","title":"Disk Quota Exceeded Errors","text":"<p>If a <code>disk quota exceeded</code> error messages is encountered, please check each storage location to ensure there is enough disk space available.</p>"},{"location":"storage/compare/","title":"Storage Comparison","text":"Home HTS LTS LTS LTS Home Directory (/home) Scratch Space (/scratch) Filesystem (/lts) Object Store (LTOS) Reference (/ref) Purpose - environment customizations and scripts  - development and storage of small personal project TEMPORARY STORAGE FOR:    raw/initial data needed for processing by  running/pending jobs  intermediate data generated by running jobs  final processed data prior to moving data to  safer location (LTS) after jobs complete Long Term Storage For:    raw/initial data such as sequencer data  finished (fully processed) data  documentation describing how to reproduce  fully processed data from initial data Long Term Storage For:    raw/initial data such as sequencer data  finished (fully processed) data  documentation describing how to reproduce  fully processed data from initial data Long Term Storage For:    Software  tools needed by  jobs for the processing of  data  reference  data such as  reference genomes and  NCBI databases Initial Size 20G 2TB per account - - 1TB per lab Increase Cost NA $7.77/TB/Month $7.77/TB/Month $7.77/TB/Month Size Limit 20G Temporary increase available as resources allow. Please provide # TBs needed and duration of need 10 TB per bucket  No limit on # of  buckets - - Access from All HTCF Nodes All HTCF Nodes Login Node Only All HTCF Nodes All HTCF Nodes Access Type Standard Filesystem Standard Filesystem Standard Filesystem  <code>/lts/&lt;lab&gt;/&lt;bucket&gt;</code> HTTP interface  compatible with (but not  using) Amazon S3 API Standard Filesystem  <code>/ref/&lt;lab&gt;/data</code> <code>/ref/&lt;lab&gt;/software</code> Est. Access Speed Slow 10+ GB/s (aggregate) 200 MB/s 1+ GB/s (aggregate) 100 MB/s (aggregate) Backup Policy Onsite  daily/weekly/monthly  snapshots as resources  allow  Offsite backup daily NO BACKUPS Onsite daily/weekly/monthly snapshots as resources allow.  Offsite backup daily. mirrored offsite. User customizable:  versioning of objects, schedule removal of old objects NO BACKUPS Cleaning Policy - See the scratch data cleaning policy - - -"},{"location":"storage/ltos/","title":"Long Term Object Storage","text":"<p>Long Term Object Storage (LTOS) is an architecture that manages data as objects, as opposed to traditional LTS which uses file systems and block storage.</p> <p>Access to LTOS is REST-based (HTTP).  Although LTOS uses a subset of the Amazon s3 API, LTOS data is stored internally, not in Amazon.</p> <p>Unlike LTS, which is only accessible from the login server, LTOS is accessible from all HTCF nodes.</p>"},{"location":"storage/ltos/#purpose","title":"Purpose","text":"<p>LTOS can be a good alternative to LTS any time the data in question doesn't need to be heavily manipulated or modified.</p> <p>Good Candidates for LTOS:</p> <ul> <li>Raw sequence data and finished analysis data</li> <li>Archived or rarely referenced data such as alumni files.</li> <li>Data that is not often modified, once created.</li> </ul> <p>Not Good Candidates for LTOS:</p> <ul> <li>scripts in use or under development</li> <li>software</li> <li>intermediate files generated during job processing</li> </ul>"},{"location":"storage/ltos/#using-ltos","title":"Using LTOS","text":""},{"location":"storage/ltos/#software-tools","title":"Software Tools","text":""},{"location":"storage/ltos/#cli","title":"CLI","text":"<p>The two most common command line tools for accessing LTOS are s3cmd and aws-cli.</p>"},{"location":"storage/ltos/#s3cmd","title":"s3cmd","text":"<p>First a configuration files needs to be created </p>"},{"location":"storage/ltos/#workflow","title":"Workflow","text":""},{"location":"storage/ltos/#putting-files-in-ltos","title":"Putting files in LTOS","text":""},{"location":"storage/ltos/#examples","title":"Examples","text":"<p>Note</p> <p>This page is a work-in-progress.</p> <p>New documentation for 2022 coming soon!</p>"},{"location":"using/","title":"Your HTCF Account","text":"<p>Once your software is set up and your data is placed in the proper storage location you are ready to get down to work.</p> <p>Work is done on the HTCF via use of the Workload Management System (or Job Scheduler).</p> <p>Slurm Scheduler Overview Here</p> <p>HTCF Layout HERE  (partitions)</p> <ul> <li> <p>batch jobs and monitoring of running batch jobs</p> </li> <li> <p>interactive jobs</p> </li> <li> <p>job accounting</p> </li> </ul>"},{"location":"using/accounting/","title":"Accounting","text":"<p>Note</p> <p>This page is a work-in-progress.</p> <p>New documentation for 2022 coming soon!</p>"},{"location":"using/batch/","title":"Batch","text":"<p>Note</p> <p>This page is a work-in-progress.</p> <p>New documentation for 2022 coming soon!</p>"},{"location":"using/getstarted/","title":"Your HTCF Account","text":""},{"location":"using/getstarted/#account-creation","title":"Account Creation","text":"<p>To request a user account on the HTCF, please send an email containing the WUSTLKey username and department ID (Workday 'CC' number), for billing purposes.</p>"},{"location":"using/getstarted/#logging-in","title":"Logging In","text":"<p>WUSTLKey credentials are used for authentication.</p> <p>The login server, <code>login.htcf.wustl.edu</code> is accessible via ssh.  </p> <p>As stated in the WUSTL and HTCF Policies, accounts and passwords cannot be shared. All users must have their own account.</p>"},{"location":"using/getstarted/#first-things-first","title":"First things first...","text":"<p>Before using the HTCF, it's important to read through and understand:</p> <ol> <li> <p>The HTCF Storage</p> </li> <li> <p>Using software on the HTCF</p> </li> </ol>"},{"location":"using/getstarted/#jobs","title":"Jobs","text":""},{"location":"using/getstarted/#resources","title":"Resources","text":"<p>The number of CPUs and MBs of RAM per node can be found using the Slurm sinfo command:</p> <pre><code>$ sinfo -N -p general -o '%n %c %m'\n</code></pre>"},{"location":"using/getstarted/#interactive","title":"Interactive","text":"<p>Interactive sessions are for running interactive scripts, vizualization, any tasks that are too computational intensive to run on the login node not submitted via sbatch.  The defaults are: 1 CPU core, 1 GB RAM, and a time limit of 8 hours.</p> <p>Note</p> <p>The HTCF is primarily a batch queuing system.</p> <p>Interactive jobs are meant to function as daily workspaces. Because interactive jobs are by their nature, inefficient, they are not meant to be running continuously for more than 1 day.</p> <p>When using interactive tools such as rstudio or jupyter, please make sure the jobs are using the \"interactive\" queue  (using sbatch/srun parameters <code>-J interactive -p interactive</code>)</p> <p>Jobs using interactive tools that are not in the interactive queue will be subject to cancellation in order to free up resources for batch jobs.</p> <p>Tools such as Rscript can be used to run R programs in a batch fashion. It appears that jupyter notebooks can also be run in a batch fashion.</p> <p>Thanks for helping to ensure fairness for all folks on the HTCF.</p> <p>An interactive session can be started using the Slurm <code>srun</code> command:</p> <pre><code>$ srun --mem-per-cpu=&lt;MBs&gt; --cpus-per-task=&lt;num&gt; -J interactive -p interactive --pty /bin/bash -l\n</code></pre>"},{"location":"using/getstarted/#batch-job-submission","title":"Batch Job Submission","text":"<ul> <li>Determine resources</li> <li>Create Job File</li> <li>Create sbatch file with required resources</li> <li>Submit</li> <li>Monitor</li> </ul>"},{"location":"using/getstarted/#workflow","title":"Workflow","text":"<p>Jobs typically follow a generic workflow.</p> <ol> <li>Preprocessed Raw Data Enters LTS</li> <li>Raw Data is copied to scratch for processing</li> <li>Post processed data is copied to LTS</li> <li>Intermediate data generated in Step 2 is removed</li> </ol>"},{"location":"using/getstarted/#sbatch-examples","title":"Sbatch Examples","text":"<p>Create a job script (myjob.sbatch): <pre><code>#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n\neval $( spack load --sh &lt;program&gt; )\n\nprogram /scratch/lab/files/ABC.fasta /scratch/lab/files/ABC.out\n</code></pre></p> <p>Submit the sbatch script:</p> <pre><code>$ sbatch myjob.sbatch\n</code></pre> <p>View the job in the queue:</p> <pre><code>$ squeue\n</code></pre>"},{"location":"using/getstarted/#gpus","title":"GPUs","text":"<p>The HTCF currently has a small number of GPUs, currently 6 NVIDIA A100 80GB and 2 V100 32GB. The default time limit of a GPU job is set to 8 hours.</p> <p>A GPU is accessible using the following slurm parameters:</p> <pre><code>-p gpu --gpus=&lt;num&gt;\n</code></pre> <p>In an sbatch:</p> <pre><code>#SBATCH -p gpu\n#SBATCH --gpus=&lt;num&gt;\n</code></pre> <p>You can verify the GPU is being utilized by the job with the nvidia-smi command, this example runs within your job allocation:</p> <p>Note</p> <p>The job must be submitted as a batch job to utilize the nvidia-smi command. </p> <pre><code>$ srun --ntasks-per-node=1 --jobid=12345 nvidia-smi\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100 80G...  Off  | 00000000:17:00.0 Off |                    0 |\n| N/A   39C    P0    62W / 300W |      0MiB / 81920MiB |     23%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0                12043      C   guppy_basecaller                 4753MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"using/interactive/","title":"Interactive","text":"<p>Note</p> <p>This page is a work-in-progress.</p> <p>New documentation for 2022 coming soon!</p>"},{"location":"using/queue/","title":"Queue","text":""},{"location":"using/queue/#queuing-system-slurm","title":"Queuing System - Slurm","text":"<p>The HTCF utlizes the Simple Linux Utility for Resource Management (Slurm).  Slurm documentation can be found at http://slurm.schedmd.com/documentation.html.</p>"},{"location":"using/queue/#job-submission","title":"Job Submission","text":"<p>There are two type of Slurm jobs, batch and interactive.</p>"},{"location":"using/queue/#batch-jobs","title":"Batch Jobs","text":"<p>The steps needed to submit batch jobs are:</p> <ol> <li>Create a \"job script\".  This is the file that actually does the work of the job.</li> <li>Create a \"sbatch script\".  This file sets the Slurm parameters and prepares the environment for the job script.</li> <li>Launch the job using the \"sbatch\" command</li> </ol> <p>salloc - Obtains a job allocation.</p> <pre><code>--cpu-per-task - Number of CPUs required per task\n--dependency=&lt;state:jobid&gt;\n--job-name=&lt;name&gt;\n--mem=&lt;MB&gt; Memory required per node.\n--mem-per-cpu=&lt;MB&gt; Memory required per allocated CPU.\n</code></pre> <p>sbatch - Submits batch scripts for execution.</p> <p>srun - Obtains job allocation and executes an application.</p>"},{"location":"using/queue/#partitions","title":"Partitions","text":"Partition Max Memory Duration Max CPUs in Queue general 250GB no limit 3004 interactive 250GB 8 hours 3004"},{"location":"using/queue/#job-management","title":"Job Management","text":""},{"location":"using/queue/#squeue","title":"squeue","text":"<p>To view your job status in the queue</p>"},{"location":"using/queue/#scancel","title":"scancel","text":"<p>Users can use scancel command to cancel their jobs or job arrays.  You may see job states of CA or CG during this processes.</p> <pre><code>scancel JOBID \nscancel -u $USER\n</code></pre>"},{"location":"using/queue/#job-accounting","title":"Job Accounting","text":"<p>sacct is the command to view all previously run job information.  You can get a list of viewable fields by running the command</p> <pre><code>sacct -e\n</code></pre> <p>To view a past jobs maximum used memory and duration <pre><code>sacct -j JOBID --format=JobID,JobName,MaxRSS,Elapsed\n</code></pre></p> <p>Scontrol can be used to view detailed information about your running job including the job script that was submitted.  Please send the output of this command if your currently running job is having issues.  <pre><code>~$ scontrol show jobid -dd 846115\nJobId=846115 JobName=sleep.sh\n   UserId=ericmartin(1002) GroupId=ericmartin(1002)\n   Priority=3070 Nice=0 Account=htcfadmin QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   DerivedExitCode=0:0\n   RunTime=00:00:09 TimeLimit=UNLIMITED TimeMin=N/A\n   SubmitTime=2016-03-09T09:47:08 EligibleTime=2016-03-09T09:47:08\n   StartTime=2016-03-09T09:47:09 EndTime=Unknown\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   Partition=general AllocNode:Sid=n082:21380\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=n082\n   BatchHost=n082\n   NumNodes=1 NumCPUs=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n     Nodes=n082 CPU_IDs=3-4 Mem=2000\n   MinCPUsNode=1 MinMemoryCPU=1000M MinTmpDiskNode=0\n   Features=(null) Gres=(null) Reservation=(null)\n   Shared=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/scratch/htcfadmin/eric/sleep.sh\n   WorkDir=/scratch/htcfadmin/eric\n   StdErr=/scratch/htcfadmin/eric/slurm-846115.out\n   StdIn=/dev/null\n   StdOut=/scratch/htcfadmin/eric/slurm-846115.out\n   BatchScript=\n#!/bin/bash\n\n#SBATCH -n 2\n#SBATCH -N 1\n\nmodule load bowtie2\n\nsleep 100\n</code></pre></p> <p>More information on usage is available at http://slurm.schedmd.com/sacct.html.</p> <p>More information available here: * http://slurm.schedmd.com/overview.html * http://slurm.schedmd.com/tutorials.html * http://slurm.schedmd.com/faq.html</p>"}]}